{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPD Quality Assurance (QA) Process\n",
    "\n",
    "## Summary\n",
    "\n",
    "This Jupyter Notebook presents a fundamental framework for conducting quality assurance on Mobile Positioning Data (MPD) with the aim of ensuring its accuracy, reliability, and suitability for further analysis.\n",
    "\n",
    "The underlying principle of quality assurance is to ascertain the adequacy of the data for subsequent analysis. Key considerations include the rate of data records per unit time, the completeness of data for each variable, and discerning data patterns.\n",
    "\n",
    "The primary objective of this notebook is to execute diverse quality assurance checks on the MPD dataset in order to guarantee its reliability and consistency.\n",
    "\n",
    "The quality assurance process will entail the examination of multiple metrics to validate the reliability and consistency of the MPD dataset. The following metrics will be scrutinized:\n",
    "\n",
    "1. Generation of user/subscriber statistics\n",
    "\n",
    "2. Evaluation of the percentage of null values for each column\n",
    "\n",
    "3. Assessment of the consistency in the number of subscribers per day\n",
    "\n",
    "4. Verification of the consistency in the number of unique cell locations\n",
    "\n",
    "5. Examination of the location of cell IDs outside the mainland\n",
    "\n",
    "6. Analysis of the distribution of active days for subscribers (day present)\n",
    "\n",
    "7. Evaluation of the diurnal distribution of subscribers' activity\n",
    "\n",
    "## Quality Assurance Metrics\n",
    "\n",
    "### 1. Generation of user/subscriber statistics\n",
    "This metric involves generating comprehensive statistics on the users and subscribers present in the MPD.\n",
    "\n",
    "### 2. Checking Percentage of Null Values\n",
    "The first metric focuses on assessing the percentage of null values in each column of the MPD dataset. By identifying the presence of null values, which indicate missing or incomplete data, we can evaluate the data completeness and take appropriate measures to address any gaps.\n",
    "\n",
    "### 3. Checking Consistency of Number of Subscribers\n",
    "This metric aims to ensure the consistency of the recorded number of subscribers on a daily basis within the MPD dataset. By examining the fluctuations or discrepancies in subscriber counts, we can identify potential data quality issues, such as data entry errors or inconsistencies in data collection processes.\n",
    "\n",
    "### 4. Checking Consistency of Number of Unique Cell Locations\n",
    "This metric examines the consistency of the count of unique cell locations within the MPD dataset. Consistent counts indicate a reliable dataset, while inconsistencies may suggest data quality issues or errors in recording cell locations. By verifying the consistency, we can ensure the accuracy and integrity of the location data.\n",
    "\n",
    "### 5. Checking the Location of Cell IDs\n",
    "This metric focuses on verifying the geographical locations associated with cell IDs in the MPD dataset. Specifically, it checks whether any cell locations are recorded outside the mainland. Identifying such outliers or data points that deviate from the expected geographical coverage helps maintain data accuracy and prevent misleading analyses.\n",
    "\n",
    "### 6. Checking the Active Day Distribution of Subscribers\n",
    "This metric involves analyzing the distribution of active days for subscribers in the MPD dataset. By examining patterns and anomalies in subscriber activity, such as variations in usage on specific days, we can gain insights into user behavior and usage trends. Understanding the distribution of active days is crucial for evaluating network reliability and optimizing resource allocation.\n",
    "\n",
    "### 7. Checking the Diurnal Distribution of Subscribers Activity\n",
    "This metric focuses on examining the diurnal distribution of subscribers' activity throughout the day within the MPD dataset. By studying peak usage hours, low-usage periods, and any abnormal patterns in subscriber activity, we can gain valuable insights into user behavior and usage patterns. Analyzing the diurnal distribution helps optimize network resources and enhance service quality.\n",
    "\n",
    "By conducting these quality assurance checks, we can ensure the reliability, accuracy, and consistency of the MPD dataset, thereby enabling more robust analysis and informed decision-making based on the mobile positioning data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Acceptance Rate\n",
    "This code defines two Python dictionaries, `CONF` and `QA_summary`, used in a quality assurance (QA) process. \n",
    "\n",
    "The `DIURNAL_REFERENCE` list contains values that define an expected distribution with an elephant shape. This is used as a reference for a QA test later on.\n",
    "\n",
    "`CONF` is initialized with a set of threshold values including:\n",
    "\n",
    "- `NULL_MAX_THRESHOLD`: the maximum number of null values allowed in a data set\n",
    "- `SUBS_MIN_THRESHOLD`: the minimum value for a substitution score in user statistics\n",
    "- `CELL_MIN_THRESHOLD`: the minimum value for a cell consistency score\n",
    "- `CELL_OOB_MAX_THRESHOLD`: the maximum percentage of out-of-bounds cells allowed\n",
    "- `ACTIVE_DAY_SKEWNESS_THRESHOLD`: the skewness threshold for an active day distribution\n",
    "- `DIURNAL_DISTRIBUTION_REFERENCE`: the `DIURNAL_REFERENCE` list defined earlier\n",
    "- `DIURNAL_DISTRIBUTION_PVALUE`: the p-value threshold for a diurnal distribution test\n",
    "\n",
    "`QA_summary` is initialized with seven different keys corresponding to different types of QA tests that will be run. The values for each key are a tuple containing two items: the status (which can be \"NOT RUN,\", \"OBSERVED\", \"PASSED,\" or \"FAILED\"), and the report (which may describe what was tested, how it was tested, and any issues found)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module to adjust the Python system path\n",
    "import sys\n",
    "\n",
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Install geopandas inside the *same environment* as the active kernel\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"geopandas\"])\n",
    "\n",
    "# Append a path to the system path so that Python can import modules from the specified directory (relative path)\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import the CONF, QA_summary and QA_action_plan variables from the script.QA module\n",
    "from script.conf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create a new directory at the path QA_PATH using os.mkdir()\n",
    "# If the directory already exists, print a message stating that it does\n",
    "try:\n",
    "    os.mkdir(QA_PATH)\n",
    "    print(\"Create new folder {}\".format(QA_PATH))\n",
    "except FileExistsError:\n",
    "    print(\"Folder {} already exists\".format(QA_PATH))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generation of user/subscriber statistics\n",
    "\n",
    "This metric involves generating comprehensive statistics on the users and subscribers present in the MPD dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas module\n",
    "import pandas as pd\n",
    "\n",
    "# Import the PySpark module\n",
    "import pyspark\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import year, month, dayofmonth, substring, col, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Import the SparkSession from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set number of core. * will takes all the available cores.\n",
    "CORE = \"*\"\n",
    "\n",
    "# Create a SparkSession with the specified configuration\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[{}]\".format(CORE))\\\n",
    "        .appName(\"02.ITU.PySpark-QA\")\\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "        .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'true')\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print the spark object which contains the SparkSession\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MPD dataset using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in parquet file using Apache Spark's DataFrame API\n",
    "df = spark.read.format('parquet')\\\n",
    "    .load(BASE_PATH+QA_FILE_PATH)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "# Print the number of records in the DataFrame\n",
    "print(\"Number of records: {}\".format(df.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the user statistics, we need to aggregate the data at the user level.\n",
    "\n",
    "The `groupBy` method groups the rows of the DataFrame by the 'msisdn' column. Then, the `agg` method aggregates the columns of the grouped Dataframe to compute several statistics for each 'msisdn'. These variables are:\n",
    "- `total_events`: Total events for each msisdn.\n",
    "- `CDR_events`: Count of CDR events for each msisdn.\n",
    "- `IPDR_events`: Count of IPDR events for each msisdn.\n",
    "- `total_date`: Total dates for each msisdn.\n",
    "- `CDR_date`: Count of CDR dates for each msisdn.\n",
    "- `IPDR_date`: Count of IPDR dates for each msisdn,\n",
    "- `start_date`: The earliest date for each msisdn.\n",
    "- `end_date`: The latest date for each msisdn.\n",
    "\n",
    "The resulting dataframe is then converted into a pandas dataframe and saved as a CSV file with semicolon separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,countDistinct, when, col, min, max \n",
    "\n",
    "# This code calculates statistics for each user and exports the results to a CSV file\n",
    "user_stats = df.groupBy('msisdn')\\\n",
    "    .agg(\n",
    "        count('datetime').alias('total_events'), # Counting total events for each msisdn\n",
    "        count(when(col('data_type')=='CDR',col('datetime'))).alias('CDR_events'), # Counting CDR events for each msisdn\n",
    "        count(when(col('data_type')=='IPDR',col('datetime'))).alias('IPDR_events'), # Counting IPDR events for each msisdn\n",
    "        countDistinct(col('date')).alias('total_date'), # Counting total dates for each msisdn\n",
    "        countDistinct(when(col('data_type')=='CDR',col('date'))).alias('CDR_date'), # Counting CDR dates for each msisdn\n",
    "        countDistinct(when(col('data_type')=='IPDR',col('date'))).alias('IPDR_date'), # Counting IPDR dates for each msisdn\n",
    "        min(col('date')).alias('start_date'), # Finding the start date for each msisdn\n",
    "        max(col('date')).alias('end_date'), # Finding the end date for each msisdn\n",
    "    )\n",
    "\n",
    "# Displays the first 5 rows of the user_stats dataframe\n",
    "user_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the user_stats dataframe to a pandas dataframe\n",
    "user_stats_df = user_stats.toPandas()\n",
    "\n",
    "# Finds the minimum start date and maximum end date for all users in the user_stats dataframe\n",
    "start_date = user_stats_df['start_date'].min()\n",
    "end_date = user_stats_df['end_date'].max()\n",
    "\n",
    "# Calculates the number of unique subscribers, the number of subscribers with CDR events, and the number of subscribers with IPDR events \n",
    "n_subs = len(user_stats_df)\n",
    "n_subs_CDR = len(user_stats_df.query('CDR_events > 0'))\n",
    "n_subs_IPDR = len(user_stats_df.query('IPDR_events > 0'))\n",
    "\n",
    "# Generates the user statistics report and saves it as text\n",
    "user_stats_report = \"\"\"Overall start date\\t\\t: {}\n",
    "Overall end date\\t\\t: {}\n",
    "Subs. with CDR & IPDR\\t: {}\n",
    "Subs. with CDR only\\t\\t: {}\n",
    "Subs. with IPDR only\\t: {}\"\"\".format(start_date,end_date,n_subs,n_subs_CDR,n_subs_IPDR)\n",
    "\n",
    "# Displays the user statistics report\n",
    "print(user_stats_report)\n",
    "\n",
    "# Adds the user statistics report to the QA_summary dictionary\n",
    "QA_summary[\"USER_STATS_REPORT\"] = (\"OBSERVED\",user_stats_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines where to save the resulting csv file\n",
    "targeted_path_csv = QA_PATH+\"1_users_stats.csv\"\n",
    "\n",
    "# Save dataframe to a csv file with tab-separated values\n",
    "user_stats_df.to_csv(targeted_path_csv, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking Percentage of Null Values\n",
    "\n",
    "This quality assurance metric focuses on identifying the percentage of null values in each column of the Mobile Positioning Data (MPD) dataset. Null values, also known as missing values, can indicate incomplete or unavailable data. It is essential to assess the extent of missing values in the dataset as they can impact subsequent analysis and decision-making processes.\n",
    "\n",
    "To perform this check, follow the steps below:\n",
    "\n",
    "1. Calculate the percentage of null values for each column.\n",
    "2. Present the results in a clear and readable format.\n",
    "\n",
    "It is recommended to visualize the percentage of null values using a bar chart or a heat map to gain a better understanding of the distribution of missing values across different columns.\n",
    "\n",
    "Make sure to have the necessary libraries installed (`PySpark, Matplotlib, Seaborn`) before running the code. Adjust the figure size and other visual settings as desired."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage of null values for each column\n",
    "\n",
    "This code calculates the percentage of null values for each column in a pandas DataFrame called df. \n",
    "\n",
    "- The first step is to calculate the total number of records in the DataFrame using the `count()` method and store it in a variable named `total_records`.\n",
    "- Next, an empty list called `null_percentages` is created which will eventually be populated with the percentage of null values for each column.\n",
    "- A for loop is then used to iterate over each column in the DataFrame using the columns attribute. Within the loop, the number of null values for each column is calculated by filtering the DataFrame on each column and using the `isNull()` method to identify null values. The count() method is then used to count the number of null values for that column and stored in a variable called null_count. The percentage of null values for that column is then calculated by dividing the null_count by the total_records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = df.count()   # count the total number of records\n",
    "\n",
    "null_percentages = []   # create an empty list to store the null percentages for each column\n",
    "\n",
    "# loop through each column of the dataframe\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(df[column].isNull()).count()  # count the number of null values in the current column\n",
    "    null_percentage = (null_count / total_records) * 100   # calculate the percentage of null values in the current column\n",
    "    null_percentages.append([column, null_percentage])    # append the column name and null percentage to the null_percentages list\n",
    "\n",
    "# create a new dataframe with the null percentages for each column\n",
    "null_percentages_df = pd.DataFrame(null_percentages, columns=[\"Column\", \"NullPercentage\"])\n",
    "\n",
    "# sort the dataframe in descending order of null percentages\n",
    "null_percentages_df = null_percentages_df.sort_values(\"NullPercentage\", ascending=False)\n",
    "\n",
    "# find the maximum null percentage value\n",
    "max_null = null_percentages_df['NullPercentage'].max()\n",
    "\n",
    "# check if the max null percentage is greater than the maximum threshold specified in the CONF dictionary\n",
    "null_report = \"\"\n",
    "\n",
    "if (max_null)>QA_CONF['NULL_MAX_THRESHOLD']:\n",
    "    null_report = (\"FAILED\",'Null values is greater than threshold: {}%.'.format(max_null))\n",
    "else:\n",
    "    null_report = (\"PASSED\",'Null values is lower than threshold: {}%.'.format(max_null))\n",
    "\n",
    "print(null_report)\n",
    "QA_summary['NULL_REPORT'] = null_report\n",
    "\n",
    "# display the entire null_percentages_df dataframe\n",
    "null_percentages_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the result using stacked bar chart\n",
    "\n",
    "This code performs the following operations:\n",
    "\n",
    "* Imports the necessary libraries for plotting charts (`seaborn` and `matplotlib.pyplot`).\n",
    "* Sets the style of the grid.\n",
    "* Creates a new figure with a specific size.\n",
    "* Uses the `sns` library to plot a stacked bar chart using the `barplot()` method. It takes the following arguments:\n",
    "\t* `x`: the column used for the horizontal axis (i.e., the null percentage values).\n",
    "\t* `y`: the column used for the vertical axis (i.e., the column names).\n",
    "\t* `data`: the DataFrame containing the data to be plotted.\n",
    "\t* `orient`: the orientation of the bars (it is set to \"h\" for horizontal bars).\n",
    "\t* `palette`: the color palette to use for the bars.\n",
    "* Sets the chart title, x-axis label, y-axis label, and the target path to save the image.\n",
    "* Saves the chart as an image file at the specified location.\n",
    "* Shows the plotted chart on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of the grid and create a new figure with a given size\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot a stacked bar chart using data from `null_percentages_df`\n",
    "# which contains columns and their corresponding null percentages\n",
    "sns.barplot(\n",
    "    x=\"NullPercentage\", \n",
    "    y=\"Column\", \n",
    "    data=null_percentages_df, \n",
    "    orient=\"h\", \n",
    "    palette=\"Set2\"\n",
    ")\n",
    "\n",
    "# Set the chart title, x-axis label, y-axis label and target path to save the image\n",
    "plt.title(\"Percentage of Null Values in Each Column\")\n",
    "plt.xlabel(\"Null Percentage (%)\")\n",
    "plt.ylabel(\"Column\")\n",
    "\n",
    "# Set the path\n",
    "targeted_path_img = QA_PATH+\"2_null_percentages.png\"\n",
    "\n",
    "# Save the chart as an image file at the specified location\n",
    "plt.savefig(targeted_path_img)\n",
    "\n",
    "# Show the plotted chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target path for storing the null_percentage data in CSV format\n",
    "targeted_path_csv = QA_PATH+\"2_null_percentages.csv\"\n",
    "\n",
    "# Export the `null_percentages_df` data to a CSV file at the specified location\n",
    "# Set `index=False` to exclude index column and `sep= \"\\t\"` to use tab-separation between columns\n",
    "null_percentages_df.to_csv(targeted_path_csv, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking Consistency of Number of Subscribers\n",
    "\n",
    "This metric aims to verify the consistency of the number of subscribers recorded each day in the MPD dataset. Inconsistent subscriber counts may indicate data quality issues, such as data entry errors or inconsistencies in data collection processes.\n",
    "\n",
    "To perform this quality check, we will follow these steps:\n",
    "\n",
    "1. Extract the relevant columns for analysis.\n",
    "2. Group the data by `date` and `data_type` then calculate the count of unique subscribers for each day.\n",
    "3. Identify any discrepancies or irregularities in the subscriber counts.\n",
    "4. Present the results into chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Select the columns for analysis\n",
    "subscriber_counts_df = df.select(\"date\", \"msisdn\", \"data_type\").dropDuplicates()\n",
    "\n",
    "# Group the dataset by 'date' and 'data_type' then count the number of unique subscribers for each day\n",
    "subscriber_counts_df = subscriber_counts_df\\\n",
    "    .groupBy('date','data_type')\\\n",
    "    .agg(\n",
    "        countDistinct('msisdn').alias('subscriber_count')\n",
    "    )\n",
    "\n",
    "# Calculate the minimum and maximum subscriber counts\n",
    "min_count = subscriber_counts_df.agg({'subscriber_count': 'min'}).collect()[0][0]\n",
    "max_count = subscriber_counts_df.agg({'subscriber_count': 'max'}).collect()[0][0]\n",
    "\n",
    "# Check if there are any discrepancies or irregularities in the subscriber counts\n",
    "user_consistency_report = \"\"\n",
    "\n",
    "if (min_count/max_count)<QA_CONF['SUBS_MIN_THRESHOLD']:\n",
    "    user_consistency_report = 'Discrepancies or irregularities found in the daily subscriber counts, within range {} - {} (ratio: {:.2f}%).'.format(min_count,max_count,min_count/max_count*100)\n",
    "    user_consistency_report = (\"FAILED\",user_consistency_report)\n",
    "else:\n",
    "    user_consistency_report = 'Daily subscriber counts are relatively consistent, within range {} - {} (ratio: {:.2f}%).'.format(min_count,max_count,min_count/max_count*100)\n",
    "    user_consistency_report = (\"PASSED\",user_consistency_report)\n",
    "\n",
    "print(user_consistency_report)\n",
    "QA_summary['USER_CONSISTENCY_REPORT'] = user_consistency_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the subscriber counts for each day\n",
    "subscriber_counts_df.orderBy('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the subscriber counts DataFrame to Pandas DataFrame for visualization\n",
    "subscriber_counts_pd = subscriber_counts_df.toPandas().sort_values('date')\n",
    "\n",
    "subscriber_counts_pd['subscriber_count_norm'] = subscriber_counts_pd['subscriber_count']/max_count\n",
    "\n",
    "# Plot the line chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=subscriber_counts_pd, x='date', y='subscriber_count_norm', hue='data_type',marker=\"o\")\n",
    "\n",
    "# Set the chart title and labels\n",
    "plt.title('Number of Subscribers Each Day - Normalized (Max: {})'.format(max_count))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Subscriber Count')\n",
    "\n",
    "# Set the path\n",
    "targeted_path_img = QA_PATH+\"3_number_of_subscribers_daily.png\"\n",
    "\n",
    "# Save the chart as an image file at the specified location\n",
    "plt.savefig(targeted_path_img)\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target path for storing the null_percentage data in CSV format\n",
    "targeted_path_csv = QA_PATH+\"3_number_of_subscribers_daily.csv\"\n",
    "\n",
    "# Export the `subscriber_counts_pd` data to a CSV file at the specified location\n",
    "# Set `index=False` to exclude index column and `sep= \"\\t\"` to use tab-separation between columns\n",
    "subscriber_counts_pd.to_csv(targeted_path_csv, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checking Consistency of the Number of Unique Cell Locations\n",
    "\n",
    "This metric aims to examine the consistency of the count of unique cell locations in the MPD dataset. Consistent counts indicate a reliable dataset, while inconsistencies may suggest data quality issues or errors in recording cell locations.\n",
    "\n",
    "### Steps:\n",
    "1. Extract the relevant columns for analysis.\n",
    "2. Group the dataset by the `date` and `data_type` column and count the number of unique cell locations ('cell_id') for each day.\n",
    "3. Analyze the count of unique cell locations for any discrepancies or irregularities.\n",
    "4. Present the results into chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Select the columns for analysis (date and number of subscribers)\n",
    "cell_location_counts_df = df.select(\"date\", \"cell_id\", \"data_type\").dropDuplicates()\n",
    "\n",
    "# Group the dataset by 'date' and count the number of unique cell location for each day\n",
    "cell_location_counts_df = cell_location_counts_df.groupBy('date','data_type').agg(countDistinct('cell_id').alias('cell_count'))\n",
    "\n",
    "# Calculate the minimum and maximum cell location counts\n",
    "min_count = cell_location_counts_df.agg({'cell_count': 'min'}).collect()[0][0]\n",
    "max_count = cell_location_counts_df.agg({'cell_count': 'max'}).collect()[0][0]\n",
    "\n",
    "# Check if there are any discrepancies or irregularities in the subscriber counts\n",
    "cell_consistency_report = \"\"\n",
    "\n",
    "if (min_count/max_count)<QA_CONF['CELL_MIN_THRESHOLD']:\n",
    "    cell_consistency_report = 'Discrepancies or irregularities found in the cell counts, within range {} - {}.'.format(min_count,max_count)\n",
    "    cell_consistency_report = (\"FAILED\",cell_consistency_report)\n",
    "else:\n",
    "    cell_consistency_report = 'Cell counts are relatively consistent, within range {} - {}.'.format(min_count,max_count)\n",
    "    cell_consistency_report = (\"PASSED\",cell_consistency_report)\n",
    "\n",
    "QA_summary[\"CELL_CONSISTENCY_REPORT\"] = cell_consistency_report\n",
    "print(\"CHECKING RESULT:\\n{}\".format(cell_consistency_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the subscriber counts DataFrame to Pandas DataFrame for visualization\n",
    "cell_location_counts_pd = cell_location_counts_df.toPandas().sort_values('date')\n",
    "\n",
    "cell_location_counts_pd['cell_count_norm'] = cell_location_counts_pd['cell_count']/max_count\n",
    "\n",
    "# Plot the line chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=cell_location_counts_pd, x='date', y='cell_count_norm', hue='data_type', marker=\"o\")\n",
    "\n",
    "# Set the chart title and labels\n",
    "plt.title('Number of Unique Cell Locations Each Day - Normalized (Max: {})'.format(max_count))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Cell Location Count')\n",
    "\n",
    "# Set the path\n",
    "targeted_path_img = QA_PATH+\"4_number_of_cells_daily.png\"\n",
    "\n",
    "# Save the chart as an image file at the specified location\n",
    "plt.savefig(targeted_path_img)\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target path for storing the null_percentage data in CSV format\n",
    "targeted_path_csv = QA_PATH+\"4_number_of_cells_daily.csv\"\n",
    "\n",
    "# Export the `cell_location_counts_pd` data to a CSV file at the specified location\n",
    "# Set `index=False` to exclude index column and `sep= \"\\t\"` to use tab-separation between columns\n",
    "cell_location_counts_pd.to_csv(targeted_path_csv, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checking the Location of Cell IDs\n",
    "\n",
    "The purpose of this metric is to analyze the location of cell IDs in the MPD dataset and identify any cell IDs that are located outside the mainland. Cell IDs outside the mainland may indicate data errors or inconsistencies, as they are expected to be within the geographical boundaries of the region of interest.\n",
    "\n",
    "### Steps:\n",
    "1. **Get the list of unique cell_id with latitude and longitude**: To analyze the location of cell IDs, we first extract a list of unique cell IDs from the MPD dataset along with their corresponding latitude and longitude coordinates. This information is essential for mapping the cell tower locations.\n",
    "\n",
    "2. **Create a folium map of cell tower locations using the `folium` package**: The next step involves visualizing the cell tower locations on a map. We utilize the `folium` package, which provides a user-friendly interface for creating interactive maps. By using the latitude and longitude coordinates obtained in the previous step, we can plot the cell tower locations on the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all rows with duplicate entries for `cell_id`, `latitude`, and `longitude` columns, then converting it into a Pandas dataframe \n",
    "df_cell = df.select(['cell_id','latitude','longitude']).dropDuplicates().toPandas()\n",
    "\n",
    "# Displaying the resulting Pandas dataframe `df_cell`.\n",
    "df_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing geopandas and shapely.geometry libraries \n",
    "import json, geopandas as gpd\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "# Providing the path to the GeoJSON file containing administrative boundaries.\n",
    "# Reading the data from the GeoJSON file into a geopandas dataframe named `adm_boundaries`\n",
    "\n",
    "# Option 1 of reading GEOJSON_FILE - requires Fiona package\n",
    "#adm_boundaries = gpd.read_file(GEOJSON_FILE)\n",
    "\n",
    "# Option 2 of reading GEOJSON_FILE\n",
    "with open(GEOJSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "features = gj[\"features\"]\n",
    "geoms = [shape(feat[\"geometry\"]) for feat in features]\n",
    "props = [feat.get(\"properties\", {}) for feat in features]\n",
    "\n",
    "adm_boundaries = gpd.GeoDataFrame(props, geometry=geoms, crs=\"EPSG:4326\")\n",
    "adm_boundaries = adm_boundaries.rename(columns={MUNICIPALITY_FIELD_NAME:'municipality'})\n",
    "adm_boundaries = adm_boundaries.rename(columns={MUNICIPALITY_MATCH_NAME:'geomatch'})\n",
    "\n",
    "# Displaying the resulting geopandas dataframe `adm_boundaries`.\n",
    "adm_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# 1) Points GeoDataFrame\n",
    "gdf_pts = gpd.GeoDataFrame(\n",
    "    df_cell.copy(),\n",
    "    geometry=gpd.points_from_xy(df_cell['longitude'], df_cell['latitude']),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# 2) Load or have your admin polygons as GeoDataFrame (must have a 'municipality' column)\n",
    "# admin_gdf = gpd.read_file(\"admin_boundaries.gpkg\")  # example\n",
    "# Ensure both are in the same CRS\n",
    "admin_gdf = adm_boundaries.to_crs(gdf_pts.crs)\n",
    "\n",
    "# 3) Spatial join (uses vectorized STRtree under the hood)\n",
    "gdf_joined = gpd.sjoin(gdf_pts, admin_gdf[['municipality','geometry','geomatch']], how='left', predicate='within')\n",
    "\n",
    "# 4) Back to pandas if needed\n",
    "df_cell = pd.DataFrame(gdf_joined.drop(columns=[\"geometry\", \"index_right\"], errors=\"ignore\"))\n",
    "\n",
    "# Displaying the resulting dataframe 'df_cell' with updated 'municipality' column.\n",
    "df_cell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the count of unique values in the 'municipality' column of df_cell \n",
    "df_cell['municipality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_cells = len(df_cell)\n",
    "OOB_cells = len(df_cell[df_cell['municipality']=='Out-of-bounds'])\n",
    "\n",
    "cell_location_report = \"\"\n",
    "if (OOB_cells/ALL_cells)>QA_CONF['CELL_OOB_MAX_THRESHOLD']:\n",
    "    cell_location_report = 'Number of cell location outside the mainland is greater than tolerancy = {}%.'.format(OOB_cells/ALL_cells*100)\n",
    "    cell_location_report = (\"FAILED\",cell_location_report)\n",
    "else:\n",
    "    cell_location_report = 'Number cell location outside the mainland is acceptable ({}%)'.format(np.round(OOB_cells/ALL_cells*100,2))\n",
    "    cell_location_report = (\"PASSED\",cell_location_report)\n",
    "\n",
    "QA_summary[\"CELL_LOCATION_REPORT\"] = cell_location_report\n",
    "print(cell_location_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target path for storing the data\n",
    "targeted_path_csv = QA_PATH+\"5_cell_administrative_location.csv\"\n",
    "\n",
    "# Export the `cell_location_counts_pd` data to a CSV file at the specified location\n",
    "# Set `index=False` to exclude index column and `sep= \"\\t\"` to use tab-separation between columns\n",
    "df_cell.to_csv(targeted_path_csv, index=False, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a folium map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the \"folium\" library to create web maps.\n",
    "import folium\n",
    "import json\n",
    "\n",
    "# Read the GeoJSON data using UTF-8 encoding\n",
    "with open(GEOJSON_FILE, 'r', encoding='utf-8') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "df_cell_used = df_cell[df_cell['municipality']!='Out-of-bounds']\n",
    "\n",
    "# Creating the overall cell location map by defining its initial location pointing to the first cell's latitude and longitude, along with setting its zoom level.\n",
    "cell_location = folium.Map(\n",
    "    location = [df_cell_used['latitude'].iloc[0], df_cell_used['longitude'].iloc[0]],\n",
    "    zoom_start = 10,\n",
    "    tiles= \"Cartodb Positron\"\n",
    ")\n",
    "\n",
    "# Adding the GeoJSON boundaries data to the cell location map.\n",
    "# The tooltip parameter displays information when hovering over the polygon.\n",
    "# The style_function changes the appearance of the polygons on the map.\n",
    "# The highlight_function highlights when a user clicks on a polygon.\n",
    "folium.GeoJson(geojson_data, name='adm_boundaries',\n",
    "               tooltip = folium.GeoJsonTooltip(fields=[MUNICIPALITY_FIELD_NAME]),\n",
    "               style_function = lambda feature: {'fillOpacity':0.15, 'weight':1},\n",
    "               highlight_function= lambda feature: {'fillColor': 'black'}\n",
    "              ).add_to(cell_location)\n",
    "\n",
    "# Plotting each cell tower on the map obtained from the dataframe 'df_cell'.\n",
    "# Iterating through each row of the dataframe and adding a marker for each cell tower.\n",
    "for i, row in df_cell_used.iterrows():\n",
    "    folium.Marker(\n",
    "        location = [row[\"latitude\"], row[\"longitude\"]], # Specifying the coordinates of the marker based on its latitude and longitude.\n",
    "        popup = row['cell_id'], # Displaying the tower ID as the popup message.\n",
    "        icon=folium.Icon(icon='signal') # Defining the icon to use for each marker (signal).\n",
    "    )\\\n",
    "    .add_to(cell_location) # Adding the marker to the map.\n",
    "\n",
    "# Finally, displaying the resultant cell location map.\n",
    "cell_location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Checking the Active Day Distribution of Subscribers (Day Present)\n",
    "\n",
    "The purpose of this metric is to analyze the distribution of active days for subscribers in the MPD dataset. By examining this distribution, we can identify patterns or anomalies in subscriber activity, such as high or low usage on specific days. Understanding the active day distribution is essential for assessing the reliability and usage patterns of the mobile network.\n",
    "\n",
    "### Steps:\n",
    "1. **Group the dataset by 'msisdn' (subscriber ID) and count the number of unique 'day' values for each subscriber**: To analyze the active day distribution, we group the dataset by the 'msisdn' column (representing each subscriber) and count the number of unique 'day' values associated with each subscriber. This will give us the number of active days for each subscriber.\n",
    "\n",
    "2. **Calculate the frequency distribution of the number of active days**: Once we have the count of active days for each subscriber, we calculate the frequency distribution of these counts. This distribution will show us the frequency (or occurrence) of different numbers of active days among subscribers.\n",
    "\n",
    "3. **Calculate the skewness of the 'active_days_count' column of the Pandas dataframe using the stats module from the scipy library**: Skewness is a statistical measure that helps assess the symmetry of a distribution. By calculating the skewness of the 'active_days_count' column in the Pandas dataframe, we can determine if the distribution of active days is skewed to one side (positively or negatively) or if it is approximately symmetrical. This assumption will be valid if we have enough data, like minimum in one month dataset.\n",
    "\n",
    "4. **Visualize the active day distribution using bar chart**: To gain a better understanding of the active day distribution, we can visualize it using a bar chart. This visualization will display the frequency of different numbers of active days among subscribers, allowing us to identify any patterns or anomalies in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Group the dataset by 'msisdn' and count the number of unique 'day' values for each subscriber\n",
    "active_days_df = df.groupBy('msisdn').agg(countDistinct('date').alias('active_days_count'))\n",
    "\n",
    "# Convert the above Spark dataframe into a Pandas dataframe\n",
    "active_days_pd = active_days_df.toPandas()\n",
    "\n",
    "# Calculate the skewness of the 'active_days_count' column of the Pandas dataframe using the stats module from scipy library\n",
    "skewness = stats.skew(active_days_pd['active_days_count'])\n",
    "\n",
    "# Check if the skewness score is less than the given threshold. If yes, set the QA report as Passed, else set it as Failed.\n",
    "if skewness<QA_CONF['ACTIVE_DAY_SKEWNESS_THRESHOLD']:\n",
    "    active_day_distribution_report = 'Day present of the subscribers is having asymmetry distribution (long-tailed) with skewness score: {:.2f}'.format(skewness)\n",
    "    active_day_distribution_report = (\"PASSED\",active_day_distribution_report)\n",
    "else:\n",
    "    active_day_distribution_report = 'Day present of the subscribers is having symmetry distribution with skewness score: {:.2f}'.format(skewness)\n",
    "    active_day_distribution_report = (\"FAILED\",active_day_distribution_report)\n",
    "\n",
    "# Set the active day distribution report as part of the QA summary dictionary\n",
    "QA_summary['ACTIVE_DAY_DISTRIBUTION_REPORT'] = active_day_distribution_report\n",
    "\n",
    "# Print the final active day distribution report\n",
    "print(active_day_distribution_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency distribution of the number of active days\n",
    "active_days_distribution_df = active_days_df.groupBy('active_days_count').count().orderBy('active_days_count')\n",
    "active_days_distribution_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the active day distribution using bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert the active_days_distribution_df DataFrame to Pandas DataFrame for visualization\n",
    "active_days_distribution_pd = active_days_distribution_df.toPandas()\n",
    "\n",
    "# Define the target path for storing the null_percentage data in CSV format\n",
    "targeted_path_csv = QA_PATH+\"6_active_day_distribution_of_subscribers.csv\"\n",
    "\n",
    "# Export the data to a CSV file at the specified location\n",
    "# Set `index=False` to exclude index column and `sep= \"\\t\"` to use tab-separation between columns\n",
    "active_days_distribution_pd.to_csv(targeted_path_csv, index=False, sep=\"\\t\")\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=active_days_distribution_pd, x='active_days_count', y='count')\n",
    "\n",
    "# Add labels inside the bars\n",
    "for index, row in active_days_distribution_pd.iterrows():\n",
    "    plt.text(row.name, row['count'], str(row['count']), ha='center', va='bottom')\n",
    "\n",
    "# Set the chart title and labels\n",
    "plt.title('Active Day Distribution of Subscribers')\n",
    "plt.xlabel('Number of Active Days')\n",
    "plt.ylabel('Number of Subscribers')\n",
    "\n",
    "# Set the path\n",
    "targeted_path_img = QA_PATH+\"6_active_day_distribution_of_subscribers.png\"\n",
    "\n",
    "# Save the chart as an image file at the specified location\n",
    "plt.savefig(targeted_path_img)\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Day Present Bar Chart\n",
    "The bar chart visualizes the active day distribution of subscribers. Each bar represents the number of subscribers with a specific number of active days. By analyzing the chart, you can identify potential patterns or anomalies in subscriber activity. Here are some points to consider:\n",
    "\n",
    "1. **Ideal Pattern**: In an ideal scenario, you would expect a long-tail distribution,  with a larger number of subscribers having a higher number of active days. This indicates a healthy and engaged subscriber base. A distribution is skewed if one of its tails is longer than the other. So we can use skewness score smaller than -0.5 (Left Skewness) to indicate the long-tail distribution with the majority of the data is concentrated on the higher active days.\n",
    "\n",
    "2. **Anomaly Patterns - Low Activity**: If there is a significant spike or high count of subscribers with very few active days (e.g., 1 or 2 days), it may indicate a group of subscribers who are less engaged or sporadic in their mobile usage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Checking the Diurnal Distribution of Subscribers Activity\n",
    "\n",
    "The purpose of this metric is to analyze the diurnal distribution of subscribers' activity throughout the day. By examining this distribution, we can identify peak usage hours, low-usage periods, or any abnormal patterns in subscribers' activity. This analysis is valuable for understanding user behavior and usage trends, which can be used to optimize network resources.\n",
    "\n",
    "### Steps:\n",
    "1. **Add a new column 'hour' to the existing dataframe 'df'**: To perform the diurnal distribution analysis, we need to extract the hour information from the timestamp data in our dataframe. By adding a new column 'hour' to the existing dataframe 'df', we can store the hour component of each timestamp.\n",
    "\n",
    "2. **Group the dataset by the 'msisdn' column and count the number of occurrences for each hour of the day**: To understand the distribution of subscriber activity throughout the day, we group the dataset by the 'msisdn' column (representing each subscriber) and count the number of occurrences for each hour of the day. This will give us the frequency of activity for each hour.\n",
    "\n",
    "3. **Perform Mann-Whitney U test to compare the diurnal distribution in the data with the reference distribution**: To determine if the diurnal distribution in our data significantly differs from a reference distribution, we use the` Mann-Whitney U test`. This statistical test helps us compare two independent samples (our diurnal distribution and the reference distribution) and assess if there is a significant difference in the distributions.\n",
    "\n",
    "4. **Visualize the diurnal distribution of subscribers activity using a bar chart**: To gain a better understanding of the diurnal distribution, we visualize it using a bar chart. This chart will display the frequency of subscribers' activity for each hour of the day, allowing us to easily identify peak usage hours, low-usage periods, and any abnormal patterns.\n",
    "\n",
    "By following these steps, we can analyze the diurnal distribution of subscribers activity and gain insights into user behavior and usage trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, hour, count, col\n",
    "\n",
    "# Add a new column 'hour' to the existing dataframe 'df'\n",
    "# This new column stores only the hour part of the 'datetime' column of 'df'.\n",
    "df = df.withColumn('hour',hour('datetime'))\\\n",
    "    .withColumn('month',month('datetime'))\n",
    "\n",
    "# Default configuration, will use all input data\n",
    "filtered_df = df\n",
    "\n",
    "# Filtered the data into specific date for more advanced analysis.\n",
    "# You can activate this by setting the start_date and end_date, then removing the command\n",
    "# start_date = \"2023-05-01\"\n",
    "# end_date = \"2023-05-03\"\n",
    "# filtered_df = df.filter(col('date').between(start_date,end_date))\n",
    "\n",
    "filtered_df.groupBy('date').count().sort('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by 'msisdn' and 'hour' and count the occurrences\n",
    "diurnal_distribution_pd = filtered_df.groupBy('hour','data_type')\\\n",
    "    .agg(count('*').alias('activity_count'))\\\n",
    "    .toPandas().sort_values('hour')\n",
    "\n",
    "# Define the target path for storing the null_percentage data in CSV format\n",
    "targeted_path_csv = QA_PATH+\"7_diurnal_distribution_of_subscribers_activity.csv\"\n",
    "\n",
    "# Export the data to a CSV file at the specified location\n",
    "# Set `index=False` to exclude index column and `sep= \"\\t\"` to use tab-separation between columns\n",
    "diurnal_distribution_pd.to_csv(targeted_path_csv, index=False, sep=\"\\t\")\n",
    "\n",
    "# Select all rows from the diurnal_distribution_pd dataframe where data_type is 'CDR'\n",
    "diurnal_cdr = diurnal_distribution_pd[diurnal_distribution_pd['data_type']=='CDR'].reset_index(drop=True)\n",
    "\n",
    "# Normalize the activity_count column in the new diurnal_cdr dataframe \n",
    "# by dividing each value with the maximum value of activity_count column\n",
    "diurnal_cdr['activity_count'] = diurnal_cdr['activity_count']/np.max(diurnal_cdr['activity_count'])\n",
    "\n",
    "# Select all rows from the diurnal_distribution_pd dataframe where data_type is 'IPDR'\n",
    "diurnal_ipdr = diurnal_distribution_pd[diurnal_distribution_pd['data_type']=='IPDR'].reset_index(drop=True)\n",
    "\n",
    "# Normalize the activity_count column in the new diurnal_ipdr dataframe \n",
    "# by dividing each value with the maximum value of activity_count column\n",
    "diurnal_ipdr['activity_count'] = diurnal_ipdr['activity_count']/np.max(diurnal_ipdr['activity_count'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Mann-Whitney U test to compare the diurnal distribution in the data with the reference distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 'stats' module from 'scipy' library\n",
    "from scipy import stats\n",
    "\n",
    "# Perform Mann-Whitney U test to compare two samples and calculate the p-value\n",
    "# for CDR's diurnal distribution against the reference distribution\n",
    "_, p_value_cdr = stats.mannwhitneyu(QA_CONF['DIURNAL_DISTRIBUTION_REFERENCE'], diurnal_cdr['activity_count'], alternative='two-sided')\n",
    "\n",
    "# Perform Mann-Whitney U test to compare two samples and calculate the p-value\n",
    "# for IPDR's diurnal distribution against the reference distribution\n",
    "_, p_value_ipdr = stats.mannwhitneyu(QA_CONF['DIURNAL_DISTRIBUTION_REFERENCE'], diurnal_ipdr['activity_count'], alternative='two-sided')\n",
    "\n",
    "# Set a flag indicating whether the check was passed or failed, and update the \n",
    "# corresponding check message string accordingly.\n",
    "flag_passed = \"PASSED\"\n",
    "if p_value_cdr<QA_CONF['DIURNAL_DISTRIBUTION_PVALUE']:\n",
    "    cdr_diurnal_check = \"Diurnal distribution from CDR is not following the elephant shape with p-value: {:.4f}\".format(p_value_cdr)\n",
    "    flag_passed = \"FAILED\"\n",
    "else:\n",
    "    cdr_diurnal_check = \"Diurnal distribution from CDR is following the elephant shape with p-value: {:.4f}\".format(p_value_cdr)\n",
    "\n",
    "if p_value_ipdr<QA_CONF['DIURNAL_DISTRIBUTION_PVALUE']:\n",
    "    ipdr_diurnal_check = \"Diurnal distribution from IPDR is not following the elephant shape with p-value: {:.4f}\".format(p_value_ipdr)\n",
    "    flag_passed = \"FAILED\"\n",
    "else:\n",
    "    ipdr_diurnal_check = \"Diurnal distribution from IPDR is following the elephant shape with p-value: {:.4f}\".format(p_value_cdr)\n",
    "\n",
    "# Combine the results of both checks and set the 'DIURNAL_DISTRIBUTION_REPORT' key in the\n",
    "# 'QA_summary' dictionary with a tuple containing the flag and combined check messages.\n",
    "diurnal_distribution_report = (flag_passed,(\"{}\\n{}\".format(cdr_diurnal_check,ipdr_diurnal_check)))\n",
    "QA_summary[\"DIURNAL_DISTRIBUTION_REPORT\"] = diurnal_distribution_report\n",
    "\n",
    "# Print the summary report for diurnal distribution check\n",
    "print(diurnal_distribution_report[0])\n",
    "print(diurnal_distribution_report[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the diurnal distribution of subscribers activity using a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot the bar chart\n",
    "# plt.figure(figsize=(12, 12))\n",
    "fig, axes = plt.subplots(2, 1,figsize=(16, 10))\n",
    "fig.suptitle('Diurnal Distribution of Subscribers Activity')\n",
    "sns.barplot(ax=axes[0],data=diurnal_ipdr, x='hour', y='activity_count',color='blue')\n",
    "axes[0].set_title(\"IPDR\")\n",
    "\n",
    "sns.barplot(ax=axes[1],data=diurnal_cdr, x='hour', y='activity_count',color='red')\n",
    "axes[1].set_title(\"CDR\")\n",
    "\n",
    "# Set the path\n",
    "targeted_path_img = QA_PATH+\"7_diurnal_distribution_of_subscribers_activity.png\"\n",
    "\n",
    "# Save the chart as an image file at the specified location\n",
    "plt.savefig(targeted_path_img)\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Diurnal Distribution Bar Plot:\n",
    "\n",
    "The bar plot represents the diurnal distribution of subscribers activity throughout the day. Each bar represents an hour of the day, and the height of the bar indicates the number of occurrences or activity count during that hour.\n",
    "\n",
    "When analyzing the bar plot, consider the following points:\n",
    "\n",
    "- **Ideal distribution**: In an ideal scenario, the diurnal distribution should follow an `\"elephant shape\"` pattern, where the activity count is highest during working hours and gradually decreases during non-working hours. This reflects typical usage patterns, with higher activity during the day and lower activity during the night.\n",
    "\n",
    "- **Anomalies and patterns**: Look for any anomalies or patterns that deviate from the expected distribution. These may include:\n",
    "\n",
    "  - `Flat pattern`: Unexpectedly similar counts of events in every hour indicates anomalies or abnormal data collection. It could be due to algorithmic data generation, record sampling, or other factors that drive unusual subscriber behavior.\n",
    "\n",
    "  - `Spikes or peaks`: Unexpectedly high activity counts during specific hours might indicate anomalies or abnormal patterns of usage. It could be due to events, promotions, or other factors that drive unusual subscriber behavior.\n",
    "\n",
    "  - `Inverse pattern`: Unexpectedly high activity counts during non-working hours indicating potential problem with the timezone setting in the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close spark connection\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the QA Report\n",
    "\n",
    "Finally, this code saves a Quality Assurance (QA) report to a file named \"MPD_QA_Report.txt\", using the current date and time in its name, then opens it in write mode. The standard output is redirected to the new file, and the header information for the QA report is printed with the current date and time. Then, the code iterates over the items in the `QA_summary` dictionary, printing out the flag value, check message, and explanation for each check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 'sys' and 'datetime' modules\n",
    "import sys \n",
    "import datetime\n",
    "\n",
    "OVERALL_QA_STATUS = True\n",
    "\n",
    "# Get the current date and time, and use it to create a unique filename for the QA report.\n",
    "current_datetime = datetime.datetime.now()\n",
    "filename = QA_PATH+\"{}_MPD_QA_Report.txt\".format(current_datetime.strftime('%Y%m%d'))\n",
    "\n",
    "# Open the file in write mode, redirect standard output to the new file,\n",
    "# and print the header information for the QA report.\n",
    "with open(filename, \"w\") as file:\n",
    "  original_stdout = sys.stdout\n",
    "  sys.stdout = file\n",
    "  print(\"-- MPD QUALITY ASSURANCE REPORT --\\nCreated Time: {}\\n\".format(current_datetime))\n",
    "\n",
    "  # Checking the overall result of QA\n",
    "  for key,values in QA_summary.items():\n",
    "    if(values[0]==\"FAILED\"):\n",
    "      OVERALL_QA_STATUS = False\n",
    "      break\n",
    "        \n",
    "  if OVERALL_QA_STATUS==True:\n",
    "    print(\"SUMMARY:\\nAll metrics passed the criteria. No metrics with status FAILED were identified during the quality assurance process.\")\n",
    "  else:\n",
    "    print(\"SUMMARY:\\nSome metrics not passed the criteria. Please check your data!!\\n\")\n",
    "    for key,values in QA_summary.items():\n",
    "      if(values[0]==\"FAILED\"):\n",
    "        print(\"- {} -> {}\".format(key,values[0]))\n",
    "  \n",
    "  # Iterate over the items in the QA_summary dictionary, and print out\n",
    "  # the flag value, check message, and explanation for each check.\n",
    "  print(\"\\n\\nDETAILED METRICS:\")\n",
    "  for key,values in QA_summary.items():\n",
    "    print(\"\\n{} -> {}\\nExplanation:\\n{}\".format(key,values[0],values[1]))\n",
    "    if(values[0]!=\"PASSED\"):\n",
    "        print(\"Action plan:\\n{}\".format(QA_action_plan[key]))\n",
    "\n",
    "  # Reset standard output back to the original stream.\n",
    "  sys.stdout = original_stdout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
