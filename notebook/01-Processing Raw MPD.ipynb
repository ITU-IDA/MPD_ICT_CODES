{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Mobile Positioning Data (MPD)\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook is used to process raw MPD. The process involves using `PySpark` to process raw mobile phone data from CSV/Parquet files. The data is read into Spark dataframes and transformed by rounding the `cell location data` and using reverse geocoding to obtain administrative location information. \n",
    "\n",
    "The data is preprocessed by `removing duplicates`, `anomaly subscribers`, and `remove random records`. Finally, the processed data is saved as a new CSV/Parquet file for further analysis. The goal of this process is to clean and transform the raw mobile phone data into a more usable format for analysis, allowing for valuable insights to be extracted from the data.\n",
    "\n",
    "In general, MPD data is processed `regularly` every month, so it is assumed that the input data provided is separate for each month. This timeframe can be `readjusted` according to work objectives and the availability of resources to process data.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- PySpark installed on the local machine / PySpark Cluster with HDFS \n",
    "\n",
    "- Required packages and dependencies installed (pyspark, pandas, geopandas, folium)\n",
    "\n",
    "- Raw mobile phone data (MPD) in CSV / Parquet Format.\n",
    "\n",
    "**To run this notebook, you must install spark locally or having access to spark cluster, and install all packages dependency.** \n",
    "\n",
    "Please take a look into the [Environment Notebook: Mac & Linux Users](https://github.com/mandes95/893SSA-2022-BDT-DKH/blob/main/notebook/00-Setup%20Environment%20%5Bmac%20%26%20linux%5D.ipynb) or [Environment Notebook: Windows Users](https://github.com/mandes95/893SSA-2022-BDT-DKH/blob/main/notebook/00-Setup%20Environment%20%5Bwindows%5D.ipynb)\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "To work with Mobile Positioning Data (MPD), the minimum required fields are listed below:\n",
    "\n",
    "### Option 1: Records/Events Data Already Merged with Cells Location Data\n",
    "\n",
    "| Field Name   | Type      | Mode     | Description                                          |\n",
    "|--------------|-----------|----------|------------------------------------------------------|\n",
    "| `msisdn`     | String    |          | Hashed subscribers identifier                        |\n",
    "| `datetime`   | Timestamp |          | Transaction date (date and hour)                     |\n",
    "| `cell_id`    | String    | NULLABLE | Hashed cell identifier                               |\n",
    "| `latitude`   | Float     |          | Latitude of Base Transceiver Station (BTS)           |\n",
    "| `longitude`  | Float     |          | Longitude of Base Transceiver Station (BTS)          |\n",
    "| `data_type`  | String    |          | Data source, can be CDR/CHG or IPDR/UPCC             |\n",
    "| `service`    | String    |          | Transaction service (4G/ 3G/ 2G)                     |\n",
    "\n",
    "### Option 2: Records Data Not Merged with Cells Location Data\n",
    "\n",
    "#### Records data\n",
    "\n",
    "| Field Name   | Type      | Mode     | Description                                          |\n",
    "|--------------|-----------|----------|------------------------------------------------------|\n",
    "| `msisdn`     | String    |          | Hashed subscribers identifier                        |\n",
    "| `datetime`   | Timestamp |          | Transaction date (date and hour)                     |\n",
    "| `cell_id`    | String    |          | Hashed cell identifier                               |\n",
    "| `data_type`  | String    |          | Data source, can be CDR/CHG or IPDR/UPCC             |\n",
    "\n",
    "#### Cells Data\n",
    "\n",
    "| Field Name   | Type      | Mode     | Description                                          |\n",
    "|--------------|-----------|----------|------------------------------------------------------|\n",
    "| `cell_id`    | String    |          | Hashed cell identifier                               |\n",
    "| `latitude`   | Float     |          | Latitude of Base Transceiver Station (BTS)           |\n",
    "| `longitude`  | Float     |          | Longitude of Base Transceiver Station (BTS)          |\n",
    "| `service`    | String    |          | Transaction service (4G/ 3G/ 2G)                     |\n",
    "\n",
    "To successfully run the processing of raw MPD using this notebook, please ensure to follow the same structure or adjust the script as needed. \n",
    "\n",
    "Moreover, additional information from the subscribers demography data will be nice to have for getting more informative information especially related to the gender and age group. \n",
    "\n",
    "**Note**: Please ensure the hashed cell ID is `consistent` with the records/events dataset so that it can be merged.\n",
    "\n",
    "### Demography of Subscribers (additional)\n",
    "\n",
    "Additionally, we can also add the user demography data (nice to have) to be merged with the records data so that it can be used for more detail analysis based on the gender or age_group of the subscribers.\n",
    "\n",
    "| Field name   | Type   | Mode     | Description                                             |\n",
    "|--------------|--------|----------|---------------------------------------------------------|\n",
    "| `msisdn`     | String |          | Hashed subscribers identifier                           |\n",
    "| `age`        | Int    |          | Subscribers age from registration data                  |\n",
    "| `gender`     | String |          | Subscribers gender (M/F) from registration data         |\n",
    "\n",
    "<br>\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. Initialize `Spark Session`, then read the `raw MPD` dataset using PySpark.\n",
    "\n",
    "2. `Transform` the cell location by rounding the latitude and longitude values to a certain decimal place (4 decimal places, equal to accuracy ~11.1 m. [Ref.](http://wiki.gis.com/wiki/index.php/Decimal_degrees)). Will be useful if the cell ID information is missing.\n",
    "\n",
    "3. `Exploratory data analysis (EDA)` that involves investigating and summarizing the main characteristics of raw MPD, such as the information about how many unique subscribers, records distribution, etc.\n",
    "\n",
    "4. Preprocess the data by `removing duplicates`, `anomaly subscribers` (e.g. machine, tourist), and `fast movers/random records`.\n",
    "\n",
    "5. `Save` the processed data as a new CSV or Parquet file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module to adjust the Python system path\n",
    "import sys\n",
    "\n",
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Install folium inside the *same environment* as the active kernel\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"folium\"])\n",
    "\n",
    "# Append a path to the system path so that Python can import modules from the specified directory (relative path)\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import the CONF, QA_summary and QA_action_plan variables from the script.QA module\n",
    "from script.conf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    flag = os.path.exists(BASE_PATH+RAW_FILE_PATH)\n",
    "    if flag:\n",
    "        USE_MERGED = True\n",
    "    else:\n",
    "        USE_MERGED = False\n",
    "        print(\"Merged file not found. Will proceed the data using option unmerged. Please update configuration of RAW_CELL_PATH and RAW_SUBS_PATH\")\n",
    "except:\n",
    "    USE_MERGED = False\n",
    "    print(\"Merged file not found. Will proceed the data using option unmerged. Please update configuration of RAW_CELL_PATH and RAW_SUBS_PATH\")\n",
    "\n",
    "print(\"Use Merged: {}\".format(USE_MERGED))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PySpark module\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import year, month, dayofmonth, substring, col, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Import the SparkSession from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if got error due to port configuration, run this script on terminal\n",
    "# sudo hostname -s 127.0.0.1\n",
    "\n",
    "# Create a SparkSession with the specified configuration local\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[{}]\".format(CORE))\\\n",
    "        .appName(\"01.ITU.PySpark-Raw\")\\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "        .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'true')\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print the spark object which contains the SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code below to connect your remote spark cluster instead.\n",
    "\n",
    "# # Import the PySpark module\n",
    "# import pyspark\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "\n",
    "# import pyspark.sql.functions as f\n",
    "# from pyspark.sql.functions import year, month, dayofmonth, substring, col, to_date\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # Import the SparkSession from PySpark\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# CLUSTER_URL = \"spark://master\"\n",
    "# PORT = \"7077\"\n",
    "\n",
    "# # Create a SparkSession with the specified configuration remote server\n",
    "# spark = SparkSession.builder\\\n",
    "#         .master(\"{}:{}\".format(CLUSTER_URL,PORT))\\\n",
    "#         .appName(\"01.ITU.PySpark-Raw\")\\\n",
    "#         .getOrCreate()\n",
    "\n",
    "# # Print the spark object which contains the SparkSession\n",
    "# spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Raw MPD\n",
    "The following code code sets a `BASE_PATH` variable to define the parent directory where data files are located. Then, it reads a specific CSV file into a `DataFrame` object using Apache Spark's `read()` method. \n",
    "\n",
    "If the available data is already in combined form (**option 1**), in this example refer to the CSV file `\"MPD_sample.csv\"`, which will be loaded from `BASE_PATH`. \n",
    "\n",
    "If the available data is still separated between records and cells (**option 2**), in this example refer to the CSV file `\"MPD_sample_records.csv\"` and `\"MPD_sample_cells.csv\"`, which will also be loaded from `BASE_PATH`. \n",
    "\n",
    "The resulting `DataFrame` is printed to display the first five rows in a tabular format, and then the total count of records in the `DataFrame` is printed using the `count()` method.\n",
    "\n",
    "### Set MPD Schema\n",
    "\n",
    "To import data using PySpark, its better to set specific classes by defines a schema for a set of mandatory columns in a data file using the `StructType()` class.\n",
    "\n",
    "For This notebook, the schema is composed of seven mandatory MPD fields, each defined as a `StructField()` object that specifies the name, data type, and whether the field can contain null values. The available data types are specified by the imported classes (i.e., `StringType()`, `FloatType()`, and `TimestampType()`).\n",
    "\n",
    "Once the `StructType()` object is created, it is assigned to the variable `MPDSchema_mandatory`, allowing it to be used elsewhere in the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if USE_MERGED:\n",
    "#   # Define a schema for a mandatory set of columns in a data file\n",
    "#   # Use StructType to create an object that defines the structure of the schema\n",
    "#   MPD_schema_mandatory = StructType([ \\\n",
    "#       StructField(\"msisdn\", StringType(), True), \\\n",
    "#       StructField(\"datetime\", TimestampType(), True), \\\n",
    "#       StructField(\"cell_id\", StringType(), True), \\\n",
    "#       StructField(\"latitude\", FloatType(), True), \\\n",
    "#       StructField(\"longitude\", FloatType(), True), \\\n",
    "#       StructField(\"data_type\", StringType(), True), \\\n",
    "#       StructField(\"service\", StringType(), True) \\\n",
    "#     ])\n",
    "\n",
    "#   # Read in a CSV file using Apache Spark's DataFrame API\n",
    "#   # Set options including delimiter and header presence\n",
    "#   # Provide a schema using MPDSchema_mandatory variable defined elsewhere\n",
    "#   # Load the specified CSV file into a DataFrame\n",
    "#   df = spark.read.format('csv')\\\n",
    "#       .options(delimiter=',')\\\n",
    "#       .option('header',True)\\\n",
    "#       .schema(MPD_schema_mandatory)\\\n",
    "#       .load(BASE_PATH+RAW_FILE_PATH)\n",
    "\n",
    "#   # Display the first five rows of the DataFrame in a tabular format\n",
    "#   df.show(5)\n",
    "\n",
    "#   # Print the number of records in the DataFrame\n",
    "#   print(\"Number of records: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, trim, regexp_replace, try_to_timestamp, lit, col\n",
    "\n",
    "if USE_MERGED:\n",
    "\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "    from pyspark.sql.functions import col, trim, regexp_replace, to_timestamp, coalesce, date_format\n",
    "\n",
    "    # 1) Schema: read datetime as STRING first (so multiple patterns can be tried)\n",
    "    MPD_schema_mandatory = StructType([\n",
    "        StructField(\"msisdn\",    StringType(), True),\n",
    "        StructField(\"datetime\",  StringType(), True),   # <- StringType here\n",
    "        StructField(\"cell_id\",   StringType(), True),\n",
    "        StructField(\"latitude\",  FloatType(),   True),\n",
    "        StructField(\"longitude\", FloatType(),   True),\n",
    "        StructField(\"data_type\", StringType(),  True),\n",
    "        StructField(\"service\",   StringType(),  True),\n",
    "    ])\n",
    "\n",
    "    df = (spark.read.format('csv')\n",
    "          .options(delimiter=',')\n",
    "          .option('header', True)\n",
    "          .schema(MPD_schema_mandatory)\n",
    "          .load(BASE_PATH + RAW_FILE_PATH))\n",
    "\n",
    "    # 2) Normalize whitespace in the datetime text\n",
    "    dt_clean = trim(regexp_replace(col(\"datetime\"), r\"\\s+\", \" \"))\n",
    "\n",
    "    # 3) Try multiple patterns (Java SimpleDateFormat, not Python!):\n",
    "    #    - 12h with/without seconds + AM/PM\n",
    "    #    - 24h with/without seconds\n",
    "    #    - common ISO-like variants (optional)\n",
    "    patterns = [\n",
    "        \"M/d/yyyy h:mm:ss a\",   # e.g., 9/22/2025 6:05:00 AM\n",
    "        \"M/d/yyyy h:mm a\",      # e.g., 9/22/2025 6:05 AM\n",
    "        \"M/d/yyyy H:mm:ss\",     # e.g., 9/22/2025 11:34:10\n",
    "        \"M/d/yyyy H:mm\",        # e.g., 9/22/2025 11:34\n",
    "        \"MM/dd/yyyy HH:mm:ss\",  # zero-padded month/day (optional)\n",
    "        \"yyyy-MM-dd HH:mm:ss\",  # common ISO without 'T'\n",
    "        \"yyyy-MM-dd'T'HH:mm:ss'Z'\",      # ISO UTC with Z\n",
    "        \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\",  # ISO UTC with millis\n",
    "    ]\n",
    "\n",
    "    #parsed = coalesce(*[to_timestamp(dt_clean, p) for p in patterns])\n",
    "    parsed = coalesce(*[try_to_timestamp(dt_clean, lit(p)) for p in patterns])\n",
    "\n",
    "    # 4) Overwrite the datetime column as proper TimestampType\n",
    "    df = df.withColumn(\"datetime\", parsed)\n",
    "\n",
    "    # Inspect\n",
    "    df.show(5)\n",
    "    print(\"Number of records: {}\".format(df.count()))\n",
    "    print(\"Unparsed rows:\", df.filter(col(\"datetime\").isNull()).count())  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Separated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_MERGED==False:\n",
    "  # Define a schema for a mandatory set of columns in a data file\n",
    "  # Use StructType to create an object that defines the structure of the schema\n",
    "\n",
    "  # records data schema\n",
    "  MPD_record_schema_mandatory = StructType([ \\\n",
    "      StructField(\"msisdn\", StringType(), True), \\\n",
    "      StructField(\"datetime\", TimestampType(), True), \\\n",
    "      StructField(\"cell_id\", StringType(), True), \\\n",
    "      StructField(\"data_type\", StringType(), True), \\\n",
    "    ])\n",
    "\n",
    "  # cells data schema\n",
    "  MPD_cell_schema_mandatory = StructType([ \\\n",
    "      StructField(\"cell_id\", StringType(), True), \\\n",
    "      StructField(\"latitude\", FloatType(), True), \\\n",
    "      StructField(\"longitude\", FloatType(), True), \\\n",
    "      StructField(\"service\", StringType(), True) \\\n",
    "    ])\n",
    "\n",
    "  # Read in a CSV file using Apache Spark's DataFrame API\n",
    "  # Set options including delimiter and header presence\n",
    "  # Load the records data using records schema\n",
    "  df_records = spark.read.format('csv')\\\n",
    "      .options(delimiter=',')\\\n",
    "      .option('header',True)\\\n",
    "      .schema(MPD_record_schema_mandatory)\\\n",
    "      .load(BASE_PATH+RAW_FILE_PATH)\n",
    "\n",
    "  # Load the cells data using cells schema\n",
    "  df_cells = spark.read.format('csv')\\\n",
    "      .options(delimiter=',')\\\n",
    "      .option('header',True)\\\n",
    "      .schema(MPD_cell_schema_mandatory)\\\n",
    "      .load(BASE_PATH+RAW_CELLS_PATH)\n",
    "\n",
    "  # Joining two dataframes using left join on cell_id\n",
    "  df = df_records.join(df_cells,df_records.cell_id ==  df_cells.cell_id,\"left\").drop(df_cells.cell_id)\n",
    "\n",
    "  # Reorder columns\n",
    "  df = df.select('msisdn','datetime','cell_id','latitude','longitude','data_type','service')\n",
    "\n",
    "  # Display the first five rows of the DataFrame in a tabular format\n",
    "  df.show(5)\n",
    "\n",
    "  # Print the number of records in the DataFrame\n",
    "  print(\"Number of records: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display column names and types\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding Decimal And Create New Column\n",
    "\n",
    "To start working with raw MPD, we need to rounding the decimal into standard precision (4 decimals, equal to 11.1 m) and create new column for identifying the date.\n",
    "\n",
    "The code takes the DataFrame `df` and uses the `.withColumn()` method to add three new columns to it. The first two columns are 'latitude' and 'longitude', and they are created by rounding the existing 'latitude' and 'longitude' columns to 4 decimal places using the `f.round()` method from the PySpark SQL functions library (`f`), and then assigning these rounded values to the new columns.\n",
    "\n",
    "The third column is 'date', which is created by converting the existing 'datetime' column to a date type using the `to_date()` function from the `pyspark.sql.functions` module, and then assigning the converted values to the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the 'latitude' column of the DataFrame `df` to 4 decimal places and create a new column called 'latitude' with the rounded values.\n",
    "# Do the same for the 'longitude' column and create a new column called 'longitude' with the rounded values.\n",
    "# Then convert the 'datetime' column to a date type and create a new column called 'date' with the converted values.\n",
    "\n",
    "df = df\\\n",
    "    .withColumn(\n",
    "        'latitude',f.round(col('latitude'),4) # round the 'latitude' column\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        'longitude',f.round(col('longitude'),4) # round the 'longitude' column\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        'date',to_date(col('datetime')) # convert the 'datetime' column to a date type\n",
    "    )\n",
    "\n",
    "# Show the first 5 rows of the updated DataFrame `df`.\n",
    "df.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Rows\n",
    "\n",
    "This code utilizes the `dropDuplicates()` method to efficiently eliminate duplicate rows from the data frame. \n",
    "By invoking this method, all duplicate rows are eradicated, leaving only a single instance of each unique row. If there are no changes observed before and after applying this operation, it signifies the absence of any duplicates in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect duplicate rows\n",
    "df_duplicates = df.groupBy(df.columns).count().filter(\"count > 1\")\n",
    "print(f\"number of duplicate rows: {df_duplicates.count()}\")\n",
    "df_duplicates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of records in the DataFrame\n",
    "print(\"Number of records before deduplication: {}\\n\".format(df.count()))\n",
    "\n",
    "# Drops the duplicate rows from the dataframe\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Display the first five rows of the DataFrame in a tabular format\n",
    "df.show(5)\n",
    "\n",
    "# Print the number of records in the DataFrame\n",
    "print(\"Number of records after deduplication: {}\".format(df.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Data in PySpark\n",
    "\n",
    "Caching data is a technique used in PySpark to improve the performance of iterative and interactive operations on large datasets. When you cache a DataFrame or RDD (Resilient Distributed Dataset), PySpark stores the data in memory and allows subsequent operations to access the data more quickly, avoiding the need to recompute or fetch the data from disk.\n",
    "\n",
    "`Caching` a dataframe improves the `performance` of subsequent operations that use this dataframe as it avoids reading the data from disk on every operation. However, caching requires **extra memory** usage and should be used judiciously to avoid memory overflow issues.\n",
    "\n",
    "When caching a DataFrame or RDD in PySpark, it's essential to understand the `active period` of the cache. The active period refers to the duration during which the data remains cached in memory and readily available for quick access.\n",
    "\n",
    "By default, PySpark caches data in memory using a storage level called `MEMORY_AND_DISK`. This storage level ensures that the data is stored in memory as long as possible, but if the memory becomes insufficient, it spills the excess data to disk to avoid out-of-memory errors.\n",
    "\n",
    "The active period of the cache starts when you explicitly cache the DataFrame or RDD using the cache() method. From that point onward, the cached data remains available for efficient retrieval by subsequent operations until one of the following events occurs:\n",
    "\n",
    "- **Explicit Unpersist**: If you invoke the `unpersist()` method on the DataFrame or RDD, the cached data is immediately removed from memory, and the active period ends. \n",
    "\n",
    "- **Spark Context Termination**: When the Spark application or Spark Context terminates, either by explicitly stopping the context or when the application finishes execution, the active period of the cache ends. At this point, all cached data is automatically unpersisted, and the memory is released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code caches the dataframe in memory or on disk, if there's enough memory to hold all partitions of the dataframe.\n",
    "df.cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "`Exploratory Data Analysis (EDA)` is a crucial process in data science that involves investigating and summarizing the `primary characteristics` of a dataset. The main objective of EDA is to gain insights into the raw data and comprehend its underlying structure, identify patterns, detect anomalies, and develop hypotheses for further analysis.\n",
    "\n",
    "When processing mobile positioning data, EDA involves examining the dataset to understand the available variables and their distributions, such as the number of unique subscribers, unique cell locations, and the distribution of records. This process can also help to identify any missing or erroneous values and diagnose general patterns within the data.\n",
    "\n",
    "EDA is an essential step in preparing mobile positioning data for subsequent analysis as it identifies potential issues that require addressing, including data cleaning or imputation, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Number of Unique Subscribers, Cell ID, Data type, & Services\n",
    "\n",
    "The following operations are performed:\n",
    "1. Four columns ('msisdn', 'cell_id', 'data_type', and 'service') are selected from the dataframe `df`, which we want to analyze.\n",
    "2. `countDistinct()` function is applied on each of these columns to get the count of unique entities present in them.\n",
    "3. The obtained values are then renamed, using `alias()` method, to better represent their count metrics.\n",
    "4. `show()` method is used to display the resulting table in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects four columns from the dataframe df and counts their distinct values to get insights about unique subscribers, cell ids, data types and services.\n",
    "df.select(\n",
    "      f.countDistinct(col('msisdn')).alias('unique_subscribers'), # Counting distinct values of 'msisdn' column and giving it an alias name 'unique_subscribers'\n",
    "      f.countDistinct(col('cell_id')).alias('unique_cell_id'), # Counting distinct values of 'cell_id' column and giving it an alias name 'unique_cell_id'\n",
    "      f.countDistinct(col('data_type')).alias('unique_data_type'), # Counting distinct values of 'data_type' column and giving it an alias name 'unique_data_type'\n",
    "      f.countDistinct(col('service')).alias('unique_service') # Counting distinct values of 'service' column and giving it an alias name 'unique_service'\n",
    "  ).show() # Displaying the resulting dataframe using show() method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the range of datetime, latitude, & Longitude\n",
    "\n",
    "This code generates the range values for datetime, latitude, and longitude columns in a given dataframe. \n",
    "\n",
    "It does this by performing the following operations:\n",
    "1. Selecting specific columns from the data frame using `select()`, with each selected value having an alias to rename it.\n",
    "2. Using PySpark's built-in SQL functions `min()` and `max()` along with PySpark's `col()` function to select the minimum and maximum values of datetime, latitude, and longitude respectively in the data frame.\n",
    "3. Renaming each of these selected minimum and maximum values as per their column name with the `alias()` method.\n",
    "4. Finally, displaying all the selected and renamed values using the `show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the range value of datetime, latitude, and longitude\n",
    "df_range = df\\\n",
    "    .select(\n",
    "        # Selects the minimum value of 'datetime' column and renames it to 'min_datetime'\n",
    "        f.min(col('datetime')).alias('min_datetime'),\n",
    "        # Selects the maximum value of 'datetime' column and renames it to 'max_datetime'\n",
    "        f.max(col('datetime')).alias('max_datetime'),\n",
    "        # Selects the minimum value of 'latitude' column and renames it to 'min_latitude'\n",
    "        f.min(col('latitude')).alias('min_latitude'),\n",
    "        # Selects the minimum value of 'longitude' column and renames it to 'min_longitude'\n",
    "        f.min(col('longitude')).alias('min_longitude'),\n",
    "        # Selects the maximum value of 'latitude' column and renames it to 'max_latitude'\n",
    "        f.max(col('latitude')).alias('max_latitude'),\n",
    "        # Selects the maximum value of 'longitude' column and renames it to 'max_longitude'\n",
    "        f.max(col('longitude')).alias('max_longitude'),\n",
    "    )\n",
    "\n",
    "df_range.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the folium library for creating interactive maps.\n",
    "import folium\n",
    "\n",
    "# Selecting specific columns from the dataframe and dropping duplicate entries based on latitude and longitude. Converting the result to a Pandas dataframe.\n",
    "df_cell_used = df.select(['cell_id','latitude','longitude']).dropDuplicates(['latitude','longitude']).toPandas()\n",
    "\n",
    "# Converting the df_range dataframe to a Pandas dataframe for further processing.\n",
    "df_range_pd = df_range.toPandas()\n",
    "\n",
    "# Defining the coordinates for the corners of a rectangular area.\n",
    "upper_left = (df_range_pd['min_latitude'].iloc[0], df_range_pd['min_longitude'].iloc[0])\n",
    "upper_right = (df_range_pd['max_latitude'].iloc[0], df_range_pd['min_longitude'].iloc[0])\n",
    "lower_left = (df_range_pd['min_latitude'].iloc[0], df_range_pd['max_longitude'].iloc[0])\n",
    "lower_right = (df_range_pd['max_latitude'].iloc[0], df_range_pd['max_longitude'].iloc[0])\n",
    "\n",
    "# Storing the corner coordinates in a list representing the edges of the rectangular area.\n",
    "edges = [upper_left, upper_right, lower_right, lower_left]\n",
    "\n",
    "# Creating a folium map centered at a specified location with a defined zoom level and using a specific tileset.\n",
    "cell_location = folium.Map(\n",
    "    location=[df_range_pd['min_latitude'].iloc[0], df_range_pd['min_longitude'].iloc[0]], # Centering the map at the minimum latitude and longitude coordinates.\n",
    "    zoom_start=10,  # Setting the initial zoom level of the map.\n",
    "    tiles=\"Cartodb Positron\"  # Using the \"Cartodb Positron\" tileset for the map.\n",
    ")\n",
    "\n",
    "# Adding a polygon to the map, representing the rectangular area defined by the edges with specified color and fill properties.\n",
    "folium.Polygon(locations=edges, color='#ff7800', fill=True, fill_color='#ffff00', fill_opacity=0.2).add_to(cell_location)\n",
    "# folium.Rectangle(bounds=edges, color='#ff7800', fill=True, fill_color='#ffff00', fill_opacity=0.2).add_to(cell_location)\n",
    "\n",
    "# Iterating through the rows of the df_cell_used dataframe and adding markers to the map for each cell tower location.\n",
    "for i, row in df_cell_used.sample(100).iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],  # Specifying the coordinates of the marker based on its latitude and longitude.\n",
    "        popup=row['cell_id'],  # Displaying the tower ID as the popup message.\n",
    "        icon=folium.Icon(icon='signal')  # Defining the icon to use for each marker (signal).\n",
    "    )\\\n",
    "    .add_to(cell_location)  # Adding the marker to the map.\n",
    "\n",
    "# Displaying the final map with all the added elements (polygon and markers).\n",
    "cell_location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Records Distribution\n",
    "\n",
    "The code below will group a dataframe called `df` by the `'msisdn'` column and computes various count functions for other columns. \n",
    "\n",
    "The resulting dataframe `df_user_dist` is created using Apache Spark SQL `groupBy()` method, which groups the data by 'msisdn' column. Then it applies several aggregate functions to each group of data using the `agg()` method. \n",
    "\n",
    "In particular, the following five aggregate functions are applied to different columns:\n",
    "- `count(col('msisdn'))`: counts the number of non-null rows in the 'msisdn' column for each group\n",
    "- `countDistinct(col('msisdn'))`: counts the number of distinct 'msisdn' values for each group\n",
    "- `countDistinct(col('cell_id'))`: counts the number of distinct 'cell_id' values for each group\n",
    "- `countDistinct(col('data_type'))`: counts the number of distinct 'data_type' values for each group\n",
    "- `countDistinct(col('service'))`: counts the number of distinct 'service' values for each group\n",
    "\n",
    "The resulting columns are then renamed to more meaningful names using the `alias()` method.\n",
    "\n",
    "Finally, the result from spark dataframe is collected into pandas dataframe by using the `toPandas()` method. This data will be used for plotting the records distribution to identify anomaly subscribers such as robot or tourist.\n",
    "\n",
    "The `.head()` method is called on the resulting pandas dataframe `df_user_dist` to display the first 5 rows of data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code groups the dataframe df by 'msisdn' column and applies various count functions for other columns.\n",
    "df_user_dist = (\n",
    "    df.groupBy(\"msisdn\")\n",
    "      .agg(\n",
    "          f.count(\"*\").alias(\"records\"),\n",
    "          f.countDistinct(\"date\").alias(\"unique_date\"),\n",
    "          (f.count(\"*\") / f.countDistinct(\"date\")).alias(\"avg_records_day\"),\n",
    "          f.countDistinct(\"cell_id\").alias(\"unique_cell_id\"),\n",
    "          f.countDistinct(\"data_type\").alias(\"unique_data_type\"),\n",
    "          f.countDistinct(\"service\").alias(\"unique_service\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "# If you really need pandas:\n",
    "df_user_dist = df_user_dist.toPandas()\n",
    "df_user_dist = df_user_dist.sort_values(\"avg_records_day\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the distribution of the records per subscribers, we can create a histogram of the 'records' column of df_user_dist data.\n",
    "\n",
    "When analyzing the distribution of the number of records in Mobile Positioning Data (MPD), it's possible to observe certain patterns that may align with `common anomalies` that often occur. These patterns can provide insights into potential issues or irregularities within the data.\n",
    "\n",
    "In a healthy dataset, the number of records may exhibit a relatively `uniform distribution` across different subscribers, locations, etc. This indicates a consistent and expected behavior of data collection. If there is a deviation from the uniform distribution, such as a significant difference in the number of records in some particular subscribers compared to the others, it may indicate an issue worth investigating. For example, a high frequency events from a subscribers in a single cell location could be due to the use of robot system to interact with mobile network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a histogram of the 'records' column of df_user_dist data frame with 20 bins\n",
    "df_user_dist['records'].hist(bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the distribution at the user level, we can explore more deeper by checking the distribution at cell_id level for each subscribers, so that we can see more granular pattern and identify the potential anomaly subscribers inside the data, such as robots or tourist.\n",
    "\n",
    "This code line performs the following tasks:\n",
    "\n",
    "- It groups `df` by unique combinations of `'msisdn'`, `'date'`, and `'cell_id'` columns using `groupBy()` function.\n",
    "- After grouping, it computes aggregation functions on each group. Here, it calculates the count of records for each unique combination of the three columns. Additionally, it counts the number of distinct entries in the 'data_type' and 'service' columns to get the number of unique data types and services in each group.\n",
    "- Finally, it collects the aggregated data from Spark dataframe into a new Pandas dataframe using `toPandas()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups the df data frame by unique combinations of 'msisdn', 'date', and 'cell_id' columns\n",
    "df_subs_cell_date = df \\\n",
    "  .groupBy(['msisdn','date','cell_id']) \\\n",
    "  .agg(\n",
    "      # Counts the total number of records for each unique value of 'msisdn' and renames that column as 'records'\n",
    "      f.count(col('msisdn')).alias('records'),\n",
    "      # Counts the number of unique 'data_type' values for each group and renames that column as 'unique_data_type'\n",
    "      f.countDistinct(col('data_type')).alias('unique_data_type'),\n",
    "      # Counts the number of unique 'service' values for each group and renames that column as 'unique_service'\n",
    "      f.countDistinct(col('service')).alias('unique_service')\n",
    "  ).toPandas() # collect data from spark into pandas\n",
    "\n",
    "# Sorts the resulting data frame by the 'records' column in descending order\n",
    "df_subs_cell_date.sort_values('records',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the 'records' column from the df_subs_patterns dataframe and plots a histogram with 100 bins\n",
    "df_subs_cell_date['records'].hist(bins=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Anomaly Subscribers\n",
    "\n",
    "#### Filtering Robot\n",
    "\n",
    "Here, `df_subs_stats` is assumed to be a dataframe containing information about subscriber statistics, and `msisdn` is assumed to be one of the columns containing phone numbers. The code determines the subscribers who have made more than 500 (or configured threshold) events in a single site at a single day (which are treated as robots), calculates their percentage, and prints it out along with the total count. Finally, it provides the list of the first 10 msisdns (phone numbers) of those subscribers.\n",
    "\n",
    "Step by step:\n",
    "1. Selects a subset of data from a pandas dataframe `df_subs_stats` where the value in the column 'max_records' is greater than 500 and assigns it to a new dataframe called `df_robots`.\n",
    "2. Calculates the number of rows in the `df_robots` dataframe and assigns it to a variable `n_robots`.\n",
    "3. Calculates the percentage of subscribers who have more than 500 records in a single site at a single day and assigns it to a variable `prop_robots`.\n",
    "4. Prints out the detection message of the detected number of subscribers and the corresponding percentage.\n",
    "5. Shows the first 10 values of the `msisdn` column from the `df_robots` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Robot Threshold: {}\".format(RAW_CONF['ROBOT_THRESHOLD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups the df_subs_patterns dataframe by 'msisdn' and calculates aggregated statistics for the 'records' column\n",
    "df_subs_cell_date_stats = df_subs_cell_date.groupby('msisdn').agg(\n",
    "    min_records=('records','min'),\n",
    "    med_records=('records','median'),\n",
    "    max_records=('records','max')\n",
    ").reset_index()\n",
    "\n",
    "# Prints out the resulting dataframe\n",
    "df_subs_cell_date_stats.sort_values('max_records',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_cell_date_stats['max_records'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows from the DataFrame `df_subs_stats` where 'max_records' is greater than the robot threshold and assign it to a new DataFrame called `df_robots`.\n",
    "df_robots = df_subs_cell_date_stats[df_subs_cell_date_stats['max_records']>RAW_CONF['ROBOT_THRESHOLD']]\n",
    "\n",
    "# Calculate the number of rows in the `df_robots` DataFrame and store the value in a variable called `n_robots`.\n",
    "n_robots = len(df_robots)\n",
    "\n",
    "# Calculate the percentage of robots (subscribers with more than the robot threshold of events in a single site at a single day) and store it in a variable called `prop_robots`.\n",
    "prop_robots = n_robots/(len(df_subs_cell_date_stats))*100\n",
    "\n",
    "# Print out detection message which shows the total number of detected robots and their percentage.\n",
    "print('Detecting {} subscribers ({:.2f}%) with more than {} events in a single site at a single day.'.format(n_robots,prop_robots,RAW_CONF[\"ROBOT_THRESHOLD\"]))\n",
    "\n",
    "# Convert the first 10 values of the 'msisdn' column from the `df_robots` DataFrame into a list and print it.\n",
    "df_robots['msisdn'].tolist()[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters out any rows from the original `df` DataFrame whose 'msisdn' values match any of the IDs listed in `df_robots`. The filtered DataFrame is assigned to `df_robot_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any rows from the DataFrame `df` that have 'msisdn' values that appear in the `anomaly_subscribers` list, and assign the resulting DataFrame to `df_filtered`.\n",
    "df_robot_filtered = df.filter(~df.msisdn.isin(df_robots['msisdn'].unique().tolist()))\n",
    "\n",
    "# Show the first 5 rows of the filtered DataFrame.\n",
    "df_robot_filtered.show(5)\n",
    "\n",
    "# Print out the count (total number of rows) in the filtered DataFrame.\n",
    "print(\"Number of records before robot filtering: {}\".format(df.count()))\n",
    "print(\"Number of records after robot filtering: {}\".format(df_robot_filtered.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Random Events\n",
    "\n",
    "This code creates a PySpark DataFrame `df_robot_filtered` and performs the following operations:\n",
    "* Creates a window specification called `w_subs_site_date` that partitions data by columns `'msisdn'`, `'cell_id'`, and `'date'`.\n",
    "* Uses the `withColumn()` method to add a new column called `'records_site'` to `df_robot_filtered`, which counts the number of records for each subscriber (`'msisdn'`) at each site (`'cell_id'`) per day (`'date'`). The `count()` function runs over the window specification `w_subs_site_date`.\n",
    "* Filters the resulting DataFrame `df_robot_notrandom_filtered` to keep only the rows where the value in the `'records_site'` column is greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Event Threshold: {}\".format(RAW_CONF['RANDOM_EVENT_THRESHOLD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a window object called `w_subs_site_date` that partitions by 'msisdn', 'cell_id', and 'date'\n",
    "# w_subs_site_date = Window.partitionBy(['msisdn', 'cell_id', 'date'])\n",
    "# Define a window object called `w_subs_site_date` that partitions by 'msisdn', 'cell_id'\n",
    "w_subs_site_date = Window.partitionBy(['msisdn', 'cell_id'])\n",
    "\n",
    "# Create a new DataFrame `df_robot_notrandom_filtered` using `df_robot_filtered`\n",
    "# Add a new column called 'records_site' using count() function with the window defined above\n",
    "df_robot_notrandom_filtered = df_robot_filtered \\\n",
    "    .withColumn('records_site', f.count('data_type').over(w_subs_site_date))\n",
    "\n",
    "# Print DataFrame and the number of records in it before filtering\n",
    "print(\"Number of records before random records filtering: {}\".format(df_robot_notrandom_filtered.count()))\n",
    "df_robot_notrandom_filtered.show(10)\n",
    "\n",
    "# Filter df_robot_notrandom_filtered to keep only rows where 'records_site' is greater than threshold\n",
    "df_robot_notrandom_filtered = df_robot_notrandom_filtered.filter(col('records_site') > RAW_CONF[\"RANDOM_EVENT_THRESHOLD\"])\n",
    "\n",
    "# Print DataFrame and the number of records in it after filtering\n",
    "df_robot_notrandom_filtered.show(10)\n",
    "\n",
    "print(\"Number of the records after random records filtering: {}\".format(df_robot_notrandom_filtered.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Tourists Subcribers - step 1\n",
    "\n",
    "In here, tourist is defined as subscribers with less than X total events for all sites in a month within our targeted area of home and work location. For example, in a given month, Subscriber \"A\" only has X-1 total events (visits to different locations) within the targeted area. Since this is less than X, Subscriber \"A\" meets the criteria for being classified as a tourist.\n",
    "\n",
    "The code selects all rows from the `df_subs_stats` DataFrame where the value in the 'max_records' column is less than TOURIST_EVENT_THRESHOLD (the configured threshold), indicating that these subscribers have performed less than TOURIST_EVENT_THRESHOLD events across all sites. It creates a new DataFrame called `df_tourists` to store these rows.\n",
    "\n",
    "It then calculates the number of rows in the `df_tourists` DataFrame (representing the total number of \"tourist\" subscribers), as well as the percentage of all subscribers that fall into this category.\n",
    "\n",
    "The exclusion of tourist data from the analysis of usual residence holds significance in ensuring an accurate representation of the local population, thereby enhancing the representativeness of the estimated percentage of internet usage in a specific area.\n",
    "\n",
    "Note: If tourist 1 filtering should not be done, set the parameters TOURIST_EVENT_THRESHOLD to 0 in the conf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tourist Threshold: {}\".format(RAW_CONF['TOURIST_EVENT_THRESHOLD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "df_robot_notrandom_filtered = df_robot_notrandom_filtered.withColumn('month',month('datetime'))\n",
    "\n",
    "df_subs_cell_month = df_robot_notrandom_filtered \\\n",
    "  .groupBy(['msisdn','month','cell_id','data_type','service']) \\\n",
    "  .agg(\n",
    "      # Counts the total number of records for each unique value of 'msisdn' and renames that column as 'records'\n",
    "      f.count(col('msisdn')).alias('records'),\n",
    "      # Counts the number of unique 'data_type' values for each group and renames that column as 'unique_data_type'\n",
    "      f.countDistinct(col('data_type')).alias('unique_data_type'),\n",
    "      # Counts the number of unique 'service' values for each group and renames that column as 'unique_service'\n",
    "      f.countDistinct(col('service')).alias('unique_service')\n",
    "  ).toPandas() # collect data from spark into pandas\n",
    "\n",
    "# Sorts the resulting data frame by the 'records' column in descending order\n",
    "df_subs_cell_month.sort_values('records',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# df_subs_cell_month[df_subs_cell_month['msisdn']=='subscribers_00005'].sort_values('records',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups the df_subs_patterns dataframe by 'msisdn' and calculates aggregated statistics for the 'records' column\n",
    "df_subs_cell_month_stats = df_subs_cell_month.groupby('msisdn').agg(\n",
    "    min_records=('records','min'),\n",
    "    med_records=('records','median'),\n",
    "    max_records=('records','max')\n",
    ").reset_index()\n",
    "\n",
    "# Prints out the resulting dataframe\n",
    "df_subs_cell_month_stats.sort_values('max_records',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows from `df_subs_cell_month_stats` to keep only those where 'max_records' is less than 7. \n",
    "df_tourists = df_subs_cell_month_stats[df_subs_cell_month_stats['max_records']<RAW_CONF['TOURIST_EVENT_THRESHOLD']]\n",
    "\n",
    "# Count the number of rows in `df_tourists`\n",
    "n_tourists = len(df_tourists)\n",
    "\n",
    "# Calculate the proportion of 'tourist' subscribers in `df_subs_cell_month_stats`\n",
    "prop_tourists = n_tourists/(len(df_subs_cell_month_stats))*100\n",
    "\n",
    "# Display a message about the number and proportion of subscribers identified as tourists,\n",
    "# using f-string formatting to insert the values of `n_tourists` and `prop_tourists` into the message.\n",
    "print('Detecting {} subscribers ({:.2f}%) with less than {} events from all sites.'.format(n_tourists,prop_tourists,RAW_CONF[\"TOURIST_EVENT_THRESHOLD\"]))\n",
    "\n",
    "# Display the first 10 elements in the 'msisdn' column of the DataFrame 'df_tourists'\n",
    "df_tourists['msisdn'].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows from `df_robot_notrandom_filtered` such that they do not contain any of the MSISDNs in `df_tourists`,\n",
    "# and assign the result to a new DataFrame called `df_all_filtered`.\n",
    "df_tourist1_filtered = df_robot_notrandom_filtered.filter(~df_robot_notrandom_filtered.msisdn.isin(df_tourists['msisdn'].unique().tolist()))\n",
    "\n",
    "# Show the first 5 records of the filtered DataFrame.\n",
    "df_tourist1_filtered.show(5)\n",
    "\n",
    "# Print the number of rows in the final filtered DataFrame.\n",
    "print(\"Number of records after all filtering: {}\".format(df_tourist1_filtered.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Tourists Subcribers - step 2\n",
    "\n",
    "This part identifies a second type of tourists. Tourist 2 is defined as subscribers with less than Y active days and less than Z active consecutive days in the entire time period within our targeted area of home and work location. \n",
    "\n",
    "For example, in a given month, Subscriber \"A\" is only active for Y-1 days and Z-1 days in a row within the targeted area. Since this is less than Y and Z, Subscriber \"A\" meets the criteria for being classified as a tourist 2.\n",
    "\n",
    "The code calculates the number of total days and the number of days in a row for each subscriber. If both of these are less than Y and Z the subscriber is added to a new DataFrame called `df_tourists_2` to store these rows.\n",
    "\n",
    "It then calculates the number of rows in the `df_tourists_2` DataFrame (representing the total number of \"tourist 2\" subscribers), as well as the percentage of all subscribers that fall into this category.\n",
    "\n",
    "The exclusion of tourist 2 data from the analysis of usual residence holds significance in ensuring an accurate representation of the local population, thereby enhancing the representativeness of the estimated percentage of internet usage in a specific area. \n",
    "\n",
    "Note: If tourist 2 filtering should not be done, set the parameters TOURIST_DAYS_THRESHOLD and TOURIST_DAYSROW_THRESHOLD to 0 in the conf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max number of days for tourist activity: {}\".format(RAW_CONF['TOURIST_DAYS_THRESHOLD']))\n",
    "print(\"Max number of consecutive days for tourist activity: {}\".format(RAW_CONF['TOURIST_DAYSROW_THRESHOLD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Get one row per msisdn per active date\n",
    "# ------------------------------------------------------------\n",
    "df_days = df.select(\"msisdn\", \"date\").distinct()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Compute total number of active days per msisdn\n",
    "# ------------------------------------------------------------\n",
    "df_total_days = df_days.groupBy(\"msisdn\") \\\n",
    "    .agg(F.count(\"*\").alias(\"total_active_days\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Compute longest streak of consecutive active days\n",
    "# ------------------------------------------------------------\n",
    "# 3a. Add day_number, row_number and group key\n",
    "w = Window.partitionBy(\"msisdn\").orderBy(\"date\")\n",
    "\n",
    "df_days2 = df_days \\\n",
    "    .withColumn(\"day_number\", F.datediff(col(\"date\"), F.lit(\"1970-01-01\"))) \\\n",
    "    .withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "    .withColumn(\"grp\", col(\"day_number\") - col(\"rn\"))\n",
    "\n",
    "# 3b. Count the size of each consecutive-day group\n",
    "df_streaks = df_days2.groupBy(\"msisdn\", \"grp\") \\\n",
    "    .agg(F.count(\"*\").alias(\"streak_len\"))\n",
    "\n",
    "# 3c. Extract the longest consecutive streak per msisdn\n",
    "df_longest_streak = df_streaks.groupBy(\"msisdn\") \\\n",
    "    .agg(F.max(\"streak_len\").alias(\"max_consecutive_days\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Combine active days + longest streak\n",
    "# ------------------------------------------------------------\n",
    "df_activity_stats = df_total_days \\\n",
    "    .join(df_longest_streak, \"msisdn\", \"inner\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Select tourists: < 7 total days AND < 7 consecutive days\n",
    "# ------------------------------------------------------------\n",
    "df_tourists_2 = df_activity_stats \\\n",
    "    .filter(\n",
    "        (col(\"total_active_days\") < RAW_CONF['TOURIST_DAYS_THRESHOLD']) &\n",
    "        (col(\"max_consecutive_days\") < RAW_CONF['TOURIST_DAYSROW_THRESHOLD'])\n",
    "    ).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in `df_tourists_2`\n",
    "n_tourists_2 = len(df_tourists_2)\n",
    "\n",
    "# Calculate the proportion of 'tourist 2' subscribers in `df_subs_cell_month_stats`\n",
    "prop_tourists_2 = n_tourists_2/(len(df_subs_cell_month_stats))*100\n",
    "\n",
    "# Display a message about the number and proportion of subscribers identified as tourists 2,\n",
    "# using f-string formatting to insert the values of `n_tourists_2` and `prop_tourists_2` into the message.\n",
    "print('Detecting {} subscribers ({:.2f}%) with less than {} active days and max {} consequtive days from all sites.'.format(n_tourists_2,prop_tourists_2,RAW_CONF[\"TOURIST_DAYS_THRESHOLD\"],RAW_CONF[\"TOURIST_DAYSROW_THRESHOLD\"]))\n",
    "\n",
    "# Display the first 10 elements in the 'msisdn' column of the DataFrame 'df_tourists'\n",
    "df_tourists_2['msisdn'].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows from `df_tourist1_filtered` such that they do not contain any of the MSISDNs in `df_tourists_2`,\n",
    "# and assign the result to a new DataFrame called `df_all_filtered`.\n",
    "df_all_filtered = df_tourist1_filtered.filter(~df_tourist1_filtered.msisdn.isin(df_tourists_2['msisdn'].unique().tolist()))\n",
    "\n",
    "# Show the first 5 records of the filtered DataFrame.\n",
    "df_all_filtered.show(5)\n",
    "\n",
    "# Print the number of rows in the final filtered DataFrame.\n",
    "print(\"Number of records after all filtering: {}\".format(df_all_filtered.count()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data for the next step\n",
    "\n",
    "Finally, after finishing all the filtering process, we can save the filtered data to be used for the next steps. \n",
    "\n",
    "The data is partitioned by the column 'msisdn'. Partitioning is the process of dividing data into multiple smaller and manageable parts based on some criteria or column(s). In this case, partitioning is done on 'msisdn', which means that each unique value of 'msisdn' will be stored in a separate subdirectory in the output folder. Partitioning often helps with faster processing of large datasets by reading/writing only necessary partition(s) instead of the whole dataset.\n",
    "\n",
    "The output data will be written to the directory specified by `'BASE_PATH'`, with the subdirectory name `'MPD_sample_synthetic_filtered'`. The exact path to the output directory depends on the value of `'BASE_PATH'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_filtered.select(\n",
    "      f.countDistinct(col('msisdn')).alias('unique_subscribers'), # Counting distinct values of 'msisdn' column and giving it an alias name 'unique_subscribers'\n",
    "      f.countDistinct(col('cell_id')).alias('unique_cell_id'), # Counting distinct values of 'cell_id' column and giving it an alias name 'unique_cell_id'\n",
    "      f.countDistinct(col('data_type')).alias('unique_data_type'), # Counting distinct values of 'data_type' column and giving it an alias name 'unique_data_type'\n",
    "      f.countDistinct(col('service')).alias('unique_service') # Counting distinct values of 'service' column and giving it an alias name 'unique_service'\n",
    "  ).show() # Displaying the resulting dataframe using show() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame `df_all_filtered` to disk in the Parquet file format using the `write` method.\n",
    "# Before write the file, we can drop the records_site and month column since it will not be used in further analysis\n",
    "# The data will be partitioned by the column 'msisdn' using the `partitionBy()` method.\n",
    "# This means that each unique value of 'msisdn' will be stored in a separate subdirectory in the output folder.\n",
    "# The output data will be written to the directory specified by 'BASE_PATH', with the subdirectory name 'MPD_sample_synthetic_filtered'.\n",
    "df_all_filtered = df_all_filtered.drop('records_site').drop('month')\n",
    "\n",
    "df_all_filtered\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .partitionBy('msisdn')\\\n",
    "    .parquet(BASE_PATH+FILTERED_FILE_PATH_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_filtered = df_all_filtered.drop('records_site').drop('month')\n",
    "\n",
    "df_all_filtered\\\n",
    "    .coalesce(1)\\\n",
    "    .write.option(\"header\",True)\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .csv(BASE_PATH+FILTERED_FILE_PATH_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
