{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark & Package Dependencies\n",
    "\n",
    "### Summary\n",
    "\n",
    "PySpark is a Python library used for working with large-scale data processing. In this tutorial, we will guide you through the process of installing PySpark on a Windows.\n",
    "\n",
    "The overall steps to install spark locally consist of:\n",
    "\n",
    "- Download and install Java Development Kit (JDK) version 8 or higher on your machine.\n",
    "\n",
    "- Download the latest version of Apache Spark from the official website (https://spark.apache.org/downloads.html).\n",
    "\n",
    "- Extract the downloaded Spark archive to a directory of your choice.\n",
    "\n",
    "- Install `PySpark`: You can install PySpark using pip, the Python package manager. Open a command prompt or terminal and type `pip install pyspark` to install the latest version of PySpark.\n",
    "\n",
    "- Test your installation: Run on python `import pyspark` to test your PySpark installation. If the installation is successful, you should be able to import the PySpark library without any errors.\n",
    "\n",
    "The recommended pre-requisite installation is Python, which is done from [here](https://www.python.org/downloads).\n",
    "\n",
    "### Windows User\n",
    "\n",
    "Here's a step-by-step tutorial to install PySpark on Windows:\n",
    "\n",
    "#### Step 1: Install Java\n",
    "\n",
    "PySpark requires Java to be installed on your machine, so the first step is to download and install Java Development Kit (JDK).\n",
    "\n",
    "1. Go to the Java SE Development Kit 8 Downloads page: https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\n",
    "\n",
    "2. Accept the license agreement by clicking the checkbox and download the appropriate JDK version for your system (32-bit or 64-bit).\n",
    "\n",
    "3. Run the downloaded .exe file and follow the instructions to install Java on your machine.\n",
    "\n",
    "4. Go to `Command Prompt` and type `java -version` to know the version and know whether it is installed or not.\n",
    "\n",
    "5. Add the Java path. Open the Start menu and search for `Environment Variables.`\n",
    "\n",
    "6. Click on `Edit the system environment variables` option.\n",
    "\n",
    "    <img src=\"../assets/images/00.config_search_bar.png\" width=\"40%\">\n",
    "\n",
    "7. Click on the `Environment Variables` button.\n",
    "\n",
    "    <img src=\"../assets/images/00.config_system_variable.png\" width=\"40%\">\n",
    "\n",
    "8. Under `System variables`, click on `New` button to create your new Environment variable.\n",
    "\n",
    "    <img src=\"../assets/images/00.config_environment_panel.png\" width=\"40%\">\n",
    "\n",
    "9. Set Variable Name as `JAVA_HOME` and your Variable Value as the Java installed path. For example lets assume the installed path is `C:\\Program Files (x86)\\Java\\jdk1.8.0_251`, then click 'OK' \n",
    "\n",
    "    <img src=\"../assets/images/00.config_java_path.png\" width=\"40%\">\n",
    "\n",
    "10. Locate the `Path` variable under `System variables` and click on `Edit`. Click on `New` and add new value as `C:\\Program Files (x86)\\Java\\jdk1.8.0_251\\bin` then click 'OK'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Download and install Apache Spark\n",
    "\n",
    "1. Go to the Apache Spark download page: https://spark.apache.org/downloads.html\n",
    "\n",
    "    <img src=\"../assets/images/00.install_spark.png\" width=\"60%\">\n",
    "\n",
    "2. Choose a Spark release version and select the package type as `Pre-built for Apache Hadoop 3.3 and later.`\n",
    "\n",
    "3. Download the .tgz file for the selected release version.\n",
    "\n",
    "4. Extract the downloaded .tgz file to a desired location on your machine, e.g., `C:\\spark`\n",
    "\n",
    "5. Create new `Environment Variables` for Spark. Enter `SPARK_HOME` as the variable name and set the path as `C:\\spark`.\n",
    "\n",
    "6. Create new `Environment Variables` for Spark. Enter `HADOOP_HOME` as the variable name and set the path as `C:\\spark`.\n",
    "\n",
    "7. Locate the `Path` variable under `System variables` and click on `Edit`. Click on `New` and add the following path: `%SPARK_HOME%\\bin`\n",
    "\n",
    "<br>\n",
    "\n",
    "You can verify spark installation on `Command Prompt` using the script below:\n",
    "```\n",
    "C:\\Users\\PC>pyspark\n",
    "```\n",
    "Once everything is successfully done, the following message is obtained.\n",
    "\n",
    "<img src=\"../assets/images/00.verify_spark_windows.png\" width=\"60%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Install Package Dependencies\n",
    "\n",
    "This part we will install the dependency packages that are used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install pandas, numpy, and geopandas, etc\n",
    "%pip install -q pandas numpy scipy tqdm geopandas matplotlib pyarrow folium seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install pyspark\n",
    "%pip install -q pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Test PySpark\n",
    "\n",
    "You can now test if PySpark is installed correctly in Python by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/09 21:28:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>01.ITU.PySpark-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcb194c74f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 21:28:33 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Import the PySpark module\n",
    "import pyspark\n",
    "\n",
    "# Import the SparkSession from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession with the specified configuration\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"01.ITU.PySpark-test\")\\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print the spark object which contains the SparkSession\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code imports the necessary modules to create a `SparkSession` in PySpark. \n",
    "\n",
    "The code creates a `SparkSession` with a specific set of configurations:\n",
    "- The master node is set to the local node.\n",
    "- The name of the application is set as \"01.ITU.PySpark-test\".\n",
    "- Arrow is enabled for faster serialization and deserialization.\n",
    "\n",
    "Finally, the code prints the `spark` object which represents the `SparkSession`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
