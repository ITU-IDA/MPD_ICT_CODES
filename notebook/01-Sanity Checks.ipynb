{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks Raw Mobile Positioning Data (MPD)\n",
    "\n",
    "## Summary\n",
    "This notebook focuses on performing sanity checks on mobile phone data using PySpark RDD. The checks are crucial for ensuring data quality and consist of verifying missing columns, character checks, data types, timestamp validity, and detecting forbidden values. The checks are executed using a line-by-line inspection approach employing the `check_line()` function.\n",
    "\n",
    "Please specify the `RAW_FILE_PATH` if using merged dataset (7 fields, combination of subs data and cells data) or set the correct path for both `RAW_SUBS_PATH` and `RAW_CELLS_PATH` if the data are still unmerged (4 fields for each)\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "To work with Mobile Positioning Data (MPD), the minimum required fields are listed below:\n",
    "\n",
    "### Option 1: Records/Events Data Already Merged with Cells Location Data\n",
    "\n",
    "| Field Name   | Type      | Mode     | Description                                          |\n",
    "|--------------|-----------|----------|------------------------------------------------------|\n",
    "| `msisdn`     | String    |          | Hashed subscribers identifier                        |\n",
    "| `datetime`   | Timestamp |          | Transaction date (date and hour)                     |\n",
    "| `cell_id`    | String    | NULLABLE | Hashed cell identifier                               |\n",
    "| `latitude`   | Float     |          | Latitude of Base Transceiver Station (BTS)           |\n",
    "| `longitude`  | Float     |          | Longitude of Base Transceiver Station (BTS)          |\n",
    "| `data_type`  | String    |          | Data source, can be CDR/CHG or IPDR/UPCC             |\n",
    "| `service`    | String    |          | Transaction service (4G/ 3G/ 2G)                     |\n",
    "\n",
    "### Option 2: Records Data Not Merged with Cells Location Data\n",
    "\n",
    "#### Subss Records data\n",
    "\n",
    "| Field Name   | Type      | Mode     | Description                                          |\n",
    "|--------------|-----------|----------|------------------------------------------------------|\n",
    "| `msisdn`     | String    |          | Hashed subscribers identifier                        |\n",
    "| `datetime`   | Timestamp |          | Transaction date (date and hour)                     |\n",
    "| `cell_id`    | String    |          | Hashed cell identifier                               |\n",
    "| `data_type`  | String    |          | Data source, can be CDR/CHG or IPDR/UPCC             |\n",
    "\n",
    "#### Cells Data\n",
    "\n",
    "| Field Name   | Type      | Mode     | Description                                          |\n",
    "|--------------|-----------|----------|------------------------------------------------------|\n",
    "| `cell_id`    | String    |          | Hashed cell identifier                               |\n",
    "| `latitude`   | Float     |          | Latitude of Base Transceiver Station (BTS)           |\n",
    "| `longitude`  | Float     |          | Longitude of Base Transceiver Station (BTS)          |\n",
    "| `service`    | String    |          | Transaction service (4G/ 3G/ 2G)                     |\n",
    "\n",
    "Please ensure to follow the same structure or adjust the script as needed and the file path already set up correctly. If your input data is still separated into several files, you need to combine them first using the `combine_csv_files` function\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- PySpark installed on the local machine / PySpark Cluster with HDFS \n",
    "\n",
    "- Required packages and dependencies installed (pyspark, pandas, geopandas, folium, tqdm)\n",
    "\n",
    "- Raw mobile phone data (MPD) in CSV / Parquet Format.\n",
    "\n",
    "\n",
    "**To run this notebook, you must install spark locally or having access to spark cluster, and install all packages dependency.** \n",
    "\n",
    "Please take a look into the [Environment Notebook: Mac & Linux Users](https://github.com/mandes95/893SSA-2022-BDT-DKH/blob/main/notebook/00-Setup%20Environment%20%5Bmac%20%26%20linux%5D.ipynb) or [Environment Notebook: Windows Users](https://github.com/mandes95/893SSA-2022-BDT-DKH/blob/main/notebook/00-Setup%20Environment%20%5Bwindows%5D.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function to combine  all CSV files with the same format in a folder into one combined CSV file.\n",
    "\n",
    "def combine_csv_files(folder_path, combined_file_name):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder into one combined CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing CSV files.\n",
    "    - combined_file_name (str): Name of the combined CSV file to be created.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Ensure folder path ends with '/'\n",
    "    if not folder_path.endswith('/'):\n",
    "        folder_path += '/'\n",
    "    \n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = glob.glob(folder_path + '*.csv')\n",
    "    \n",
    "    # Initialize an empty list to hold all data frames\n",
    "    df_list = []\n",
    "    \n",
    "    # Iterate over the list of CSV files\n",
    "    for csv_file in csv_files:\n",
    "        # Read each CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along the rows axis\n",
    "    combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Write the combined DataFrame to a single CSV file\n",
    "    combined_df.to_csv(folder_path + combined_file_name, index=False)\n",
    "    \n",
    "    print(f'Combined CSV file successfully saved as {combined_file_name} in {folder_path}')\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have multiple CSV files in a folder named 'data_files'\n",
    "# and you want to combine them into a single file named 'combined_data.csv'\n",
    "\n",
    "# folder_path = '../data/00_Input/'\n",
    "# combined_file_name = '../combined_data.csv'\n",
    "\n",
    "# combine_csv_files(folder_path, combined_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the sys module to adjust the Python system path\n",
    "import sys\n",
    "\n",
    "# Append a path to the system path so that Python can import modules from the specified directory (relative path)\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import the CONF, QA_summary and QA_action_plan variables from the script.QA module\n",
    "from script.conf import *\n",
    "\n",
    "# (Temporary) Replace the configuration here or in script/conf.py if needed \n",
    "# option 1\n",
    "RAW_FILE_PATH = \"mpd_synthetic_data_ken_100subs_sanity.csv\"  # File path for the raw data file for checking\n",
    "#RAW_FILE_PATH = \"combined_data.csv\"  # File path for the raw data file\n",
    "\n",
    "# option 2\n",
    "#RAW_SUBS_PATH = \"mpd_synthetic_data_20240604_v3_adjust_dirty_subs.csv\"  # File path for the raw subs data file\n",
    "#RAW_CELLS_PATH = \"mpd_synthetic_data_20240604_v3_adjust_dirty_cells.csv\"  # File path for the raw cells data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Check if the merged file exists\n",
    "    flag = os.path.exists(BASE_PATH+RAW_FILE_PATH)\n",
    "    if flag:\n",
    "        USE_MERGED = True\n",
    "        SANITY_FILE_PATH = RAW_FILE_PATH.replace(\".csv\",\"_sanity.jsonl\")\n",
    "\n",
    "    else:\n",
    "        USE_MERGED = False\n",
    "        SANITY_FILE_PATH = [RAW_SUBS_PATH.replace(\".csv\",\"_sanity.jsonl\"),RAW_CELLS_PATH.replace(\".csv\",\"_sanity.jsonl\")]\n",
    "        print(\"Merge file not found. Will proceed the data using option unmerge. Please update configuration of RAW_CELL_PATH and RAW_SUBS_PATH\")\n",
    "\n",
    "except:\n",
    "    USE_MERGED = False\n",
    "    SANITY_FILE_PATH = [RAW_SUBS_PATH.replace(\".csv\",\"_sanity.jsonl\"),RAW_CELLS_PATH.replace(\".csv\",\"_sanity.jsonl\")]\n",
    "    print(\"Merge file not found. Will proceed the data using option unmerge. Please update configuration of RAW_CELL_PATH and RAW_SUBS_PATH\")\n",
    "\n",
    "# Check if merged is set to False\n",
    "if USE_MERGED == False:\n",
    "    try:\n",
    "        # Check if both RAW_SUBS_PATH and RAW_CELLS_PATH exist\n",
    "        flag = os.path.exists(BASE_PATH+RAW_SUBS_PATH) and os.path.exists(BASE_PATH+RAW_CELLS_PATH)\n",
    "        if flag==False:\n",
    "            USE_MERGED = None\n",
    "            SANITY_FILE_PATH = None\n",
    "            print(\"File(s) not found. Please check your input path again\")\n",
    "    except FileExistsError:\n",
    "        USE_MERGED = None\n",
    "        SANITY_FILE_PATH = None\n",
    "        print(\"File(s) not found. Please check your input path again\")\n",
    "\n",
    "print(\"Use Merge: {}\".format(USE_MERGED))\n",
    "print(f\"Target output path: {SANITY_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Try to create a new directory at the path QA_PATH using os.mkdir()\n",
    "# If the directory already exists, print a message stating that it does\n",
    "try:\n",
    "    os.mkdir(SANITY_PATH)\n",
    "    print(\"Create new folder {}\".format(SANITY_PATH))\n",
    "except FileExistsError:\n",
    "    print(\"Folder {} already exists\".format(SANITY_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the PySpark module\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import year, month, dayofmonth, substring, col, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Import the SparkSession from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "CORE = 1\n",
    "# if got error due to port configuration, run this script on terminal\n",
    "# sudo hostname -s 127.0.0.1\n",
    "# Create a SparkSession with the specified configuration local\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[{}]\".format(CORE))\\\n",
    "        .appName(\"01.ITU.PySpark-Sanity-Checks\")\\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "        .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'true')\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print the spark object which contains the SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code below to connect your remote spark cluster instead.\n",
    "\n",
    "# # Import the PySpark module\n",
    "# import pyspark\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "\n",
    "# import pyspark.sql.functions as f\n",
    "# from pyspark.sql.functions import year, month, dayofmonth, substring, col, to_date\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # Import the SparkSession from PySpark\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# CLUSTER_URL = \"spark://master\"\n",
    "# PORT = \"7077\"\n",
    "\n",
    "# # Create a SparkSession with the specified configuration remote server\n",
    "# spark = SparkSession.builder\\\n",
    "#         .master(\"{}:{}\".format(CLUSTER_URL,PORT))\\\n",
    "#         .appName(\"01.ITU.PySpark-Raw\")\\\n",
    "#         .getOrCreate()\n",
    "\n",
    "# # Print the spark object which contains the SparkSession\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Checker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visible_chars(value):\n",
    "    # Define a regex pattern:\n",
    "    # - [^\\w\\s,.;:'\"@#$*()+=\\-/] for characters that are not alphanumeric, common punctuations, or certain symbols\n",
    "    # - [\\?!] specifically include question mark and exclamation mark as invalid characters\n",
    "    # - \\x00-\\x1F,\\x7F-\\xFF includes control and non-standard ASCII characters (extended ASCII)\n",
    "    pattern = r\"[^\\w\\s,.;:'\\\"@#$*()+=\\-/]+|[\\?!]|\\x00-\\x1F|\\x7F-\\xFF\"\n",
    "\n",
    "    # Search for the regex pattern in the given value\n",
    "    return re.search(pattern,value)\n",
    "\n",
    "def is_valid_timestamp(timestamp):\n",
    "    try:\n",
    "        if isinstance(timestamp, datetime):\n",
    "            return True\n",
    "\n",
    "        # Collapse extra spaces and coerce to str\n",
    "        val = \" \".join(str(timestamp).split())\n",
    "\n",
    "        # Try your configured format first (if present), then a set of common variants\n",
    "        formats = []\n",
    "        try:\n",
    "            formats.append(datetime_format)  # your existing format\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        # Common US-style variants:\n",
    "        # - 12h w/ seconds, AM/PM\n",
    "        # - 12h w/o seconds, AM/PM\n",
    "        # - 24h w/ seconds\n",
    "        # - 24h w/o seconds   <-- this covers '9/22/2025 11:34'\n",
    "        formats.extend([\n",
    "            \"%m/%d/%Y %I:%M:%S %p\",\n",
    "            \"%m/%d/%Y %I:%M %p\",\n",
    "            \"%m/%d/%Y %H:%M:%S\",\n",
    "            \"%m/%d/%Y %H:%M\",\n",
    "        ])\n",
    "\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                datetime.strptime(val, fmt)\n",
    "                return True\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_valid_datatype(value, data_type):\n",
    "    try:\n",
    "        if data_type == datetime:\n",
    "            # Already a datetime? it's valid.\n",
    "            if isinstance(value, datetime):\n",
    "                return True\n",
    "\n",
    "            # Normalize whitespace and coerce to string\n",
    "            val = \" \".join(str(value).split())\n",
    "\n",
    "            # Try your existing format first (if defined), then common variants\n",
    "            datetime_formats = []\n",
    "            try:\n",
    "                datetime_formats.append(datetime_format)  # your configured format\n",
    "            except NameError:\n",
    "                pass\n",
    "\n",
    "            datetime_formats.extend([\n",
    "                \"%m/%d/%Y %I:%M:%S %p\",  # e.g., 9/22/2025 6:05:00 AM\n",
    "                \"%m/%d/%Y %I:%M %p\",     # e.g., 9/22/2025 6:05 AM\n",
    "                \"%m/%d/%Y %H:%M:%S\",     # e.g., 9/22/2025 11:34:10\n",
    "                \"%m/%d/%Y %H:%M\",        # e.g., 9/22/2025 11:34\n",
    "            ])\n",
    "\n",
    "            for fmt in datetime_formats:\n",
    "                try:\n",
    "                    datetime.strptime(val, fmt)\n",
    "                    return True\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "            return False\n",
    "        else:\n",
    "            data_type(value)\n",
    "            return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def listing_issues(case,value,verbose=True):\n",
    "    # It takes three parameters: case, value, and verbose (which is set to True by default)\n",
    "    # Check if verbose is True\n",
    "    if verbose:\n",
    "        # If verbose is True, return a dictionary with keys 'case' and 'value' and their corresponding values\n",
    "        return {'case':case,'value':value}\n",
    "    else:\n",
    "        # If verbose is False, return a dictionary with keys 'case' and 'value' and their corresponding values\n",
    "        return {'case':case}\n",
    "\n",
    "def check_issues(issues,size):\n",
    "    # Iterate over the first 10 items in detected_issues list for showcase\n",
    "    for issue in issues[:size]:\n",
    "        # Check if the 'case_type' key exists in the issue dictionary and its value is truthy\n",
    "        if issue['case_type']:\n",
    "            # Print the row number, issues detected, and the associated case type\n",
    "            print(f\"Row {issue['row']}: Issues detected -> {issue['case_type']}\")\n",
    "\n",
    "def write_issues(issues,output_path):\n",
    "    parent_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    # It takes two parameters: issues (a list of dictionaries) and output_path (the path to the output file)\n",
    "    # Open the output file in write mode, using a context manager to ensure proper file handling\n",
    "    with open(output_path, 'w') as f:\n",
    "        # Iterate over each issue in the issues list\n",
    "        for issue in tqdm(issues):\n",
    "            # Check if the 'case_type' key of the issue dictionary has a length greater than 0\n",
    "            if len(issue['case_type'])>0:\n",
    "                # If the condition is satisfied, write the issue dictionary to the file as JSON\n",
    "                json.dump(issue, f)\n",
    "                # Write a new line character to separate each issue\n",
    "                f.write('\\n')\n",
    "                \n",
    "                \n",
    "def check_line(line_index, line, field_data_types, skip_header=True, verbose=False):\n",
    "    # It takes several parameters: line_index (the index of the current line), line (the contents of the line), \n",
    "    # expected_num_fields (the expected number of fields in each line), field_data_types (a dictionary specifying the data types for each field),\n",
    "    # skip_header (a boolean flag indicating whether to skip the header line, default is True), and verbose (a boolean flag indicating whether to print detailed information, default is False)\n",
    "    # Create an empty list to store any issues encountered while checking the line\n",
    "    issues = list()\n",
    "    expected_num_fields = len(field_data_types)\n",
    "\n",
    "    # Check if the header line should be skipped and if the current line is the first line\n",
    "    if skip_header & (line_index == 0):\n",
    "        # Return a dictionary containing the row index and the issues list\n",
    "        return {'row': line_index, 'case_type': issues}\n",
    "    \n",
    "    # Case 1: Check for missing columns\n",
    "    if len(line) != expected_num_fields:\n",
    "        # Create a new issue using the listing_issues() function, passing the appropriate arguments\n",
    "        # The case argument is set to 'Case 1 - Missing Column', and the value argument is set to the current line\n",
    "        # The verbose argument is passed to the listing_issues() function as well\n",
    "        issues.append(listing_issues(case='Case 1 - Missing Column', value=line, verbose=verbose))\n",
    "\n",
    "    # Cases 2, 3, 4, 5, 6 : Check characters, data types, timestamp, forbidden value, missing value\n",
    "    for i, value in enumerate(line): # i =  col index, value = col value\n",
    "        \n",
    "        if i >= expected_num_fields:  # Skip if index exceeds the predefined types\n",
    "            continue\n",
    "        \n",
    "        field_name, expected_type, example_value = field_data_types[i]\n",
    "        \n",
    "        # Detect hidden or inappropriate characters (non-printable or control characters)\n",
    "        if visible_chars(value):\n",
    "            issues.append(listing_issues(case=f'Case 2 - Hidden Characters: {field_name}',value=value,verbose=verbose))\n",
    "\n",
    "        # Check data types consistency\n",
    "        if not is_valid_datatype(value, expected_type):\n",
    "            issues.append(listing_issues(case=f'Case 3 - Inconsistent Data Type: {field_name}',value=value,verbose=verbose))\n",
    "        \n",
    "        # Check for impossible timestamp\n",
    "        if expected_type == datetime and not is_valid_timestamp(value):\n",
    "            issues.append(listing_issues(case='Case 4 - Impossible Timestamp',value=value,verbose=verbose))\n",
    "\n",
    "        # Check invalid value\n",
    "        if example_value is not None:\n",
    "            try:\n",
    "                _value = expected_type(value)\n",
    "                if (expected_type in [str]) and (_value not in example_value):\n",
    "                    issues.append(listing_issues(case=f'Case 5 - Invalid Value: {field_name}',value=value,verbose=verbose))\n",
    "                \n",
    "                if (expected_type in [int,float]) and ((_value < example_value[0]) or (_value > example_value[1])):\n",
    "                    issues.append(listing_issues(case=f'Case 5 - Invalid Value: {field_name}',value=value,verbose=verbose))\n",
    "            except:\n",
    "                \"invalid\"\n",
    "\n",
    "        # Check if missing value\n",
    "        if value == \"\":\n",
    "            issues.append(listing_issues(case=f'Case 6 - Missing Value: {field_name}',value=value,verbose=verbose))\n",
    "\n",
    "    return {'row': line_index, 'case_type': issues}\n",
    "\n",
    "def detect_data_issues(lines_with_index, field_data_types, skip_header=True,verbose=False):\n",
    "    # The function detect_data_issues takes multiple parameters:\n",
    "    # - lines_with_index: A collection of lines with their respective indices\n",
    "    # - expected_num_fields: The number of fields expected in each line\n",
    "    # - field_data_types: A list specifying the data type of each field\n",
    "    # - skip_header: A boolean indicating whether to skip the header line (default is True)\n",
    "    # - verbose: A boolean indicating whether to print additional information (default is False)\n",
    "\n",
    "    # The function uses the map method on 'lines_with_index' and applies a lambda function to each line.\n",
    "    # The lambda function calls another function 'check_line' and passes it the necessary arguments.\n",
    "    # Finally, the collect method is used to retrieve the result as a list.\n",
    "    return lines_with_index.map(lambda line: check_line(line[0], line[1], field_data_types,skip_header,verbose)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Raw MPD Line by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library\n",
    "import re  # Importing the regular expression module\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime  # Importing the datetime module from the datetime library\n",
    "import json  # Importing the JSON module\n",
    "from statistics import mode\n",
    "\n",
    "# configuration\n",
    "delimiter = \",\"  # The delimiter character used to split lines\n",
    "datetime_format = \"%Y-%m-%d %H:%M:%S\"  # The format of the datetime value\n",
    "#datetime_format = \"%Y-%m-%d %H:%M\"  # The format of the datetime value\n",
    "\n",
    "# opt 1 fields\n",
    "field_data_types = [\n",
    "    ('msisdn',str, None),         # Field name, data type, and additional constraints\n",
    "    ('datetime',datetime, None),  # Field name, data type, and additional constraints\n",
    "    ('cell_id',str, None),        # Field name, data type, and additional constraints\n",
    "    ('latitude',float, [-90.0,90.0]),  # Field name, data type, and additional constraints (latitude)\n",
    "    ('longitude',float, [-180.0,180.0]),  # Field name, data type, and additional constraints (longitude)\n",
    "    ('data_type',str, [\"CDR\",\"IPDR\"]),   # Field name, data type, and allowed values for data_type\n",
    "    ('service',str, [\"2G\",\"3G\",\"4G\",\"5G\"])  # Field name, data type, and allowed values for service\n",
    "]\n",
    "\n",
    "# opt 2 fields\n",
    "field_data_types_subs = [field_data_types[i] for i in [0,1,2,5]]  # Subset of field_data_types for subs dataset\n",
    "field_data_types_cells = [field_data_types[i] for i in [2,3,4,6]]  # Subset of field_data_types for cells dataset\n",
    "\n",
    "if USE_MERGED==True: #opt 1\n",
    "    # Load a text file from the given path using Spark's context\n",
    "    # Note: The following variables need to be defined in order for this line to work:\n",
    "    #   - spark: Spark context object\n",
    "    #   - BASE_PATH: Base path of the file\n",
    "    #   - RAW_FILE_PATH: Path of the raw file\n",
    "    print(\"Start Checking MPD file\")\n",
    "    lines = spark.sparkContext.textFile(BASE_PATH + RAW_FILE_PATH)\n",
    "    field_len = lines.map(lambda line: len(line.split(delimiter))).collect()\n",
    "    field_len = mode(field_len)\n",
    "    print(f\"fields len: {field_len}\")\n",
    "\n",
    "    # Zip each line with its corresponding index, then split each line by the specified delimiter\n",
    "    indexed_lines = lines.zipWithIndex().map(lambda line: (line[1], line[0].split(delimiter)))\n",
    "    detected_issues = detect_data_issues(indexed_lines, field_data_types, verbose=True)\n",
    "    check_issues(detected_issues,10)\n",
    "    write_issues(detected_issues, SANITY_PATH+\"/\"+SANITY_FILE_PATH)\n",
    "    \n",
    "elif USE_MERGED==False:\n",
    "    # check subs file\n",
    "    print(\"Start Checking Subs file\")\n",
    "    lines = spark.sparkContext.textFile(BASE_PATH + RAW_SUBS_PATH)\n",
    "    field_len = lines.map(lambda line: len(line.split(delimiter))).collect()\n",
    "    field_len = mode(field_len)\n",
    "    print(f\"fields len: {field_len}\")\n",
    "\n",
    "    # Zip each line with its corresponding index, then split each line by the specified delimiter\n",
    "    indexed_lines = lines.zipWithIndex().map(lambda line: (line[1], line[0].split(delimiter)))\n",
    "    detected_issues = detect_data_issues(indexed_lines, field_data_types_subs, verbose=True)\n",
    "    check_issues(detected_issues,10)\n",
    "    write_issues(detected_issues, SANITY_PATH+\"/\"+SANITY_FILE_PATH[0])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # check cell file\n",
    "    print(\"Start Checking Cells file\")\n",
    "    lines = spark.sparkContext.textFile(BASE_PATH + RAW_CELLS_PATH)\n",
    "    field_len = lines.map(lambda line: len(line.split(delimiter))).collect()\n",
    "    field_len = mode(field_len)\n",
    "    print(f\"fields len: {field_len}\")\n",
    "\n",
    "    # Zip each line with its corresponding index, then split each line by the specified delimiter\n",
    "    indexed_lines = lines.zipWithIndex().map(lambda line: (line[1], line[0].split(delimiter)))\n",
    "    detected_issues = detect_data_issues(indexed_lines, field_data_types_cells, verbose=True)\n",
    "    check_issues(detected_issues,10)\n",
    "    write_issues(detected_issues, SANITY_PATH+\"/\"+SANITY_FILE_PATH[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark:Python",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
