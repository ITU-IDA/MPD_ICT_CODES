{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark & Package Dependencies\n",
    "\n",
    "### Summary\n",
    "\n",
    "PySpark is a Python library used for working with large-scale data processing. In this tutorial, we will guide you through the process of installing PySpark on a Macbook/Linux.\n",
    "\n",
    "The overall steps to install spark locally consist of:\n",
    "\n",
    "- Download and install Java Development Kit (JDK) version 8 or higher on your machine.\n",
    "\n",
    "- Download the latest version of Apache Spark from the official website (https://spark.apache.org/downloads.html).\n",
    "\n",
    "- Extract the downloaded Spark archive to a directory of your choice.\n",
    "\n",
    "- Install `PySpark`: You can install PySpark using pip, the Python package manager. Open a command prompt or terminal and type `pip install pyspark` to install the latest version of PySpark.\n",
    "\n",
    "- Test your installation: Run on python `import pyspark` to test your PySpark installation. If the installation is successful, you should be able to import the PySpark library without any errors.\n",
    "\n",
    "### MacOS and Linux User\n",
    "\n",
    "Before we begin, we can simplify the installation process by using `Homebrew`. Homebrew (brew) is a free and `open-source` package manager that allows installing apps and software in macOS, depending on the userâ€™s desire. It has been recommended for its simplicity and effectiveness in saving time and effort. Its famous description is `The missing package manager for macOS` (tested on macOS Monterey 12.6.5). Homebrew can also be used in Linux OS. \n",
    "\n",
    "#### Step 1: Install Homebrew\n",
    "\n",
    "To install Homebrew, you can use the following command in the terminal:\n",
    "```\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "```\n",
    "<img src=\"../assets/images/00.install_brew.png\" width=\"40%\">\n",
    "\n",
    "verify:\n",
    "\n",
    "```\n",
    "brew --version\n",
    "```\n",
    "<img src=\"../assets/images/00.verify_brew.png\" width=\"50%\">\n",
    "\n",
    "#### Step 2: Install Java, Scala, & Apache Spark\n",
    "\n",
    "To install Java, Scala, and Apache Spark on macOS using Homebrew, you can use the following command in the terminal:\n",
    "\n",
    "```\n",
    "brew install java scala apache-spark\n",
    "```\n",
    "\n",
    "This will install the latest versions of Java, Scala, and Apache Spark.\n",
    "\n",
    "After that, you need to create a symbolic link from the openjdk.jdk directory installed by Homebrew to the standard JDK installation directory on macOS using the command in the terminal:\n",
    "\n",
    "```\n",
    "sudo ln -sfn $(brew --prefix)/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk\n",
    "```\n",
    "\n",
    "You can verify spark installation on terminal using the script below:\n",
    "```\n",
    "spark-shell --version\n",
    "```\n",
    "Once everything is successfully done, the following message is obtained.\n",
    "\n",
    "<img src=\"../assets/images/00.verify_spark.png\" width=\"50%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Install Package Dependencies\n",
    "\n",
    "This part we will install the dependency packages that are used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install pandas, numpy, and geopandas, etc\n",
    "%pip install -q pandas numpy scipy tqdm geopandas matplotlib pyarrow folium seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install pyspark\n",
    "%pip install -q pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Test PySpark\n",
    "\n",
    "You can now test if PySpark is installed correctly in Python by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/09 21:28:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>01.ITU.PySpark-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcb194c74f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 21:28:33 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Import the PySpark module\n",
    "import pyspark\n",
    "\n",
    "# Import the SparkSession from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession with the specified configuration\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"01.ITU.PySpark-test\")\\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print the spark object which contains the SparkSession\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code imports the necessary modules to create a `SparkSession` in PySpark. \n",
    "\n",
    "The code creates a `SparkSession` with a specific set of configurations:\n",
    "- The master node is set to the local node.\n",
    "- The name of the application is set as \"01.ITU.PySpark-test\".\n",
    "- Arrow is enabled for faster serialization and deserialization.\n",
    "\n",
    "Finally, the code prints the `spark` object which represents the `SparkSession`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
